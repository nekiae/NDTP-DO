coder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.
        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        speaker_embeddings (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`, *optional*):
            Tensor containing the speaker embeddings.
        labels (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_mel_bins)`, *optional*):
            Float values of target mel spectrogram. Spectrograms can be obtained using [`SpeechT5Processor`]. See
            [`SpeechT5Processor.__call__`] for details.
        stop_labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Binary tensor indicating the position of the stop token in the sequence.

        Example:

        ```python
        >>> from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan, set_seed
        >>> from datasets import load_dataset
        >>> import torch

        >>> dataset = load_dataset(
        ...     "hf-internal-testing/librispeech_asr_demo", "clean", split="validation", trust_remote_code=True
        ... )  # doctest: +IGNORE_RESULT
        >>> dataset = dataset.sort("id")
        >>> sampling_rate = dataset.features["audio"].sampling_rate

        >>> processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_vc")
        >>> model = SpeechT5ForSpeechToSpeech.from_pretrained("microsoft/speecht5_vc")
        >>> vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")

        >>> # audio file is decoded on the fly
        >>> inputs = processor(audio=dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

        >>> speaker_embeddings = torch.zeros((1, 512))  # or load xvectors from a file

        >>> set_seed(555)  # make deterministic

        >>> # generate speech
        >>> speech = model.generate_speech(inputs["input_values"], speaker_embeddings, vocoder=vocoder)
        >>> speech.shape
        torch.Size([77824])
        ```
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if labels is not None:
            if decoder_input_values is None:
                decoder_input_values, decoder_attention_mask = shift_spectrograms_right(
                    labels, self.config.reduction_factor, decoder_attention_mask
                )

        outputs = self.speecht5(
            input_values=input_values,
            attention_mask=attention_mask,
            decoder_input_values=decoder_input_values,
            decoder_attention_mask=decoder_attention_mask,
            head_mask=head_mask,
            decoder_head_mask=decoder_head_mask,
            cross_attn_head_mask=cross_attn_head_mask,
            encoder_outputs=encoder_outputs,
            past_key_values=past_key_values,
            use_cache=use_cache,
            speaker_embeddings=speaker_embeddings,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
        )

        _, spectrogram, logits = self.speech_decoder_postnet(outputs[0])

        loss = None

        if not return_dict:
            output = (spectrogram,) + outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return Seq2SeqSpectrogramOutput(
            loss=loss,
            spectrogram=spectrogram,
            past_key_values=outputs.past_key_values,
            decoder_hidden_states=outputs.decoder_hidden_states,
            decoder_attentions=outputs.decoder_attentions,
            cross_attentions=outputs.cross_attentions,
            encoder_last_hidden_state=outputs.encoder_last_hidden_state,
            encoder_hidden_states=outputs.encoder_hidden_states,
            encoder_attentions=outputs.encoder_attentions,
        )

    @torch.no_grad()
    def generate_speech(
        self,
        input_values: torch.FloatTensor,
        speaker_embeddings: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        threshold: float = 0.5,
        minlenratio: float = 0.0,
        maxlenratio: float = 20.0,
        vocoder: Optional[nn.Module] = None,
        output_cross_attentions: bool = False,
        return_output_lengths: bool = False,
    ) -> torch.FloatTensor:
        r"""
        Converts a raw speech waveform into a sequence of mel spectrograms, which are subsequently turned back into a
        speech waveform using a vocoder.

        Args:
            input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
                Float values of input raw speech waveform.

                Values can be obtained by loading a *.flac* or *.wav* audio file into an array of type `List[float]` or
                a `numpy.ndarray`, *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
                into `input_values`, the [`SpeechT5Processor`] should be used for padding and conversion into a tensor
                of type `torch.FloatTensor`. See [`SpeechT5Processor.__call__`] for details.
            speaker_embeddings (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`, *optional*):
                Tensor containing the speaker embeddings.
            attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing convolution and attention on padding token indices. Mask values selected in
                `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

                [What are attention masks?](../glossary#attention-mask)
            threshold (`float`, *optional*, defaults to 0.5):
                The generated sequence ends when the predicted stop token probability exceeds this value.
            minlenratio (`float`, *optional*, defaults to 0.0):
                Used to calculate the minimum required length for the output sequence.
            maxlenratio (`float`, *optional*, defaults to 20.0):
                Used to calculate the maximum allowed length for the output sequence.
            vocoder (`nn.Module`, *optional*, defaults to `None`):
                The vocoder that converts the mel spectrogram into a speech waveform. If `None`, the output is the mel
                spectrogram.
            output_cross_attentions (`bool`, *optional*, defaults to `False`):
                Whether or not to return the attentions tensors of the decoder's cross-attention layers.
            return_output_lengths (`bool`, *optional*, defaults to `False`):
                Whether or not to return the concrete spectrogram/waveform lengths.

        Returns:
            `tuple(torch.FloatTensor)` comprising various elements depending on the inputs:
            - when `return_output_lengths` is False
                - **spectrogram** (*optional*, returned when no `vocoder` is provided) `torch.FloatTensor` of shape
                `(output_sequence_length, config.num_mel_bins)` -- The predicted log-mel spectrogram.
                - **waveform** (*optional*, returned when a `vocoder` is provided) `torch.FloatTensor` of shape
                `(num_frames,)` -- The predicted speech waveform.
                - **cross_attentions** (*optional*, returned when `output_cross_attentions` is `True`)
                `torch.FloatTensor` of shape `(config.decoder_layers, config.decoder_attention_heads,
                output_sequence_length, input_sequence_length)` -- The outputs of the decoder's cross-attention layers.
            - when `return_output_lengths` is True
                - **spectrograms** (*optional*, returned when no `vocoder` is provided) `torch.FloatTensor` of shape
                `(batch_size, output_sequence_length, config.num_mel_bins)` -- The predicted log-mel spectrograms that
                are padded to the maximum length.
                - **spectrogram_lengths** (*optional*, returned when no `vocoder` is provided) `List[Int]` -- A list of
                all the concrete lengths for each spectrogram.
                - **waveforms** (*optional*, returned when a `vocoder` is provided) `torch.FloatTensor` of shape
                `(batch_size, num_frames)` -- The predicted speech waveforms that are padded to the maximum length.
                - **waveform_lengths** (*optional*, returned when a `vocoder` is provided) `List[Int]` -- A list of all
                the concrete lengths for each waveform.
                - **cross_attentions** (*optional*, returned when `output_cross_attentions` is `True`)
                `torch.FloatTensor` of shape `(batch_size, config.decoder_layers, config.decoder_attention_heads,
                output_sequence_length, input_sequence_length)` -- The outputs of the decoder's cross-attention layers.
        """
        if speaker_embeddings is None:
            speaker_embeddings = torch.zeros((1, 512), device=input_values.device)

        return _generate_speech(
            self,
            input_values,
            speaker_embeddings,
            attention_mask,
            threshold,
            minlenratio,
            maxlenratio,
            vocoder,
            output_cross_attentions,
            return_output_lengths,
        )


class HifiGanResidualBlock(nn.Module):
    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5), leaky_relu_slope=0.1):
        super().__init__()
        self.leaky_relu_slope = leaky_relu_slope

        self.convs1 = nn.ModuleList(
            [
                nn.Conv1d(
                    channels,
                    channels,
                    kernel_size,
                    stride=1,
                    dilation=dilation[i],
                    padding=self.get_padding(kernel_size, dilation[i]),
                )
                for i in range(len(dilation))
            ]
        )
        self.convs2 = nn.ModuleList(
            [
                nn.Conv1d(
                    channels,
                    channels,
                    kernel_size,
                    stride=1,
                    dilation=1,
                    padding=self.get_padding(kernel_size, 1),
                )
                for _ in range(len(dilation))
            ]
        )

    def get_padding(self, kernel_size, dilation=1):
        return (kernel_size * dilation - dilation) // 2

    def apply_weight_norm(self):
        weight_norm = nn.utils.weight_norm
        if hasattr(nn.utils.parametrizations, "weight_norm"):
            weight_norm = nn.utils.parametrizations.weight_norm

        for layer in self.convs1:
            weight_norm(layer)
        for layer in self.convs2:
            weight_norm(layer)

    def remove_weight_norm(self):
        for layer in self.convs1:
            nn.utils.remove_weight_norm(layer)
        for layer in self.convs2:
            nn.utils.remove_weight_norm(layer)

    def forward(self, hidden_states):
        for conv1, conv2 in zip(self.convs1, self.convs2):
            residual = hidden_states
            hidden_states = nn.functional.leaky_relu(hidden_states, self.leaky_relu_slope)
            hidden_states = conv1(hidden_states)
            hidden_states = nn.functional.leaky_relu(hidden_states, self.leaky_relu_slope)
            hidden_states = conv2(hidden_states)
            hidden_states = hidden_states + residual
        return hidden_states


@auto_docstring(
    custom_intro="""
    HiFi-GAN vocoder.
    """
)
class SpeechT5HifiGan(PreTrainedModel):
    config_class = SpeechT5HifiGanConfig
    main_input_name = "spectrogram"

    def __init__(self, config: SpeechT5HifiGanConfig):
        super().__init__(config)
        self.num_kernels = len(config.resblock_kernel_sizes)
        self.num_upsamples = len(config.upsample_rates)
        self.conv_pre = nn.Conv1d(
            config.model_in_dim,
            config.upsample_initial_channel,
            kernel_size=7,
            stride=1,
            padding=3,
        )

        self.upsampler = nn.ModuleList()
        for i, (upsample_rate, kernel_size) in enumerate(zip(config.upsample_rates, config.upsample_kernel_sizes)):
            self.upsampler.append(
                nn.ConvTranspose1d(
                    config.upsample_initial_channel // (2**i),
                    config.upsample_initial_channel // (2 ** (i + 1)),
                    kernel_size=kernel_size,
                    stride=upsample_rate,
                    padding=(kernel_size - upsample_rate) // 2,
                )
            )

        self.resblocks = nn.ModuleList()
        for i in range(len(self.upsampler)):
            channels = config.upsample_initial_channel // (2 ** (i + 1))
            for kernel_size, dilation in zip(config.resblock_kernel_sizes, config.resblock_dilation_sizes):
                self.resblocks.append(HifiGanResidualBlock(channels, kernel_size, dilation, config.leaky_relu_slope))

        self.conv_post = nn.Conv1d(channels, 1, kernel_size=7, stride=1, padding=3)

        self.register_buffer("mean", torch.zeros(config.model_in_dim))
        self.register_buffer("scale", torch.ones(config.model_in_dim))

        # Initialize weights and apply final processing
        self.post_init()

    def _init_weights(self, module):
        """Initialize the weights."""
        if isinstance(module, (nn.Linear, nn.Conv1d)):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()

    def apply_weight_norm(self):
        weight_norm = nn.utils.weight_norm
        if hasattr(nn.utils.parametrizations, "weight_norm"):
            weight_norm = nn.utils.parametrizations.weight_norm

        weight_norm(self.conv_pre)
        for layer in self.upsampler:
            weight_norm(layer)
        for layer in self.resblocks:
            layer.apply_weight_norm()
        weight_norm(self.conv_post)

    def remove_weight_norm(self):
        nn.utils.remove_weight_norm(self.conv_pre)
        for layer in self.upsampler:
            nn.utils.remove_weight_norm(layer)
        for layer in self.resblocks:
            layer.remove_weight_norm()
        nn.utils.remove_weight_norm(self.conv_post)

    @auto_docstring(
        custom_intro="""
        Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch
        of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech
        waveform.
        """
    )
    def forward(self, spectrogram: torch.FloatTensor) -> torch.FloatTensor:
        r"""
        spectrogram (`torch.FloatTensor`):
            Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,
            config.model_in_dim)`, or un-batched and of shape `(sequence_length, config.model_in_dim)`.

        Returns:
            `torch.FloatTensor`: Tensor containing the speech waveform. If the input spectrogram is batched, will be of
            shape `(batch_size, num_frames,)`. If un-batched, will be of shape `(num_frames,)`.
        """
        if self.config.normalize_before:
            spectrogram = (spectrogram - self.mean) / self.scale

        is_batched = spectrogram.dim() == 3
        if not is_batched:
            spectrogram = spectrogram.unsqueeze(0)

        hidden_states = spectrogram.transpose(2, 1)

        hidden_states = self.conv_pre(hidden_states)
        for i in range(self.num_upsamples):
            hidden_states = nn.functional.leaky_relu(hidden_states, self.config.leaky_relu_slope)
            hidden_states = self.upsampler[i](hidden_states)

            res_state = self.resblocks[i * self.num_kernels](hidden_states)
            for j in range(1, self.num_kernels):
                res_state += self.resblocks[i * self.num_kernels + j](hidden_states)
            hidden_states = res_state / self.num_kernels

        hidden_states = nn.functional.leaky_relu(hidden_states)
        hidden_states = self.conv_post(hidden_states)
        hidden_states = torch.tanh(hidden_states)

        if not is_batched:
            # remove batch dim and collapse tensor to 1-d audio waveform
            waveform = hidden_states.squeeze(0).transpose(1, 0).view(-1)
        else:
            # remove seq-len dim since this collapses to 1
            waveform = hidden_states.squeeze(1)

        return waveform


__all__ = [
    "SpeechT5ForSpeechToText",
    "SpeechT5ForSpeechToSpeech",
    "SpeechT5ForTextToSpeech",
    "SpeechT5Model",
    "SpeechT5PreTrainedModel",
    "SpeechT5HifiGan",
]
          l m _ l o g i t s   =   s e l f . l m _ h e a d ( o u t p u t s [ 0 ] ) 
 
                 m a s k e d _ l m _ l o s s   =   N o n e 
                 i f   l a b e l s   i s   n o t   N o n e : 
                         l o s s _ f c t   =   C r o s s E n t r o p y L o s s ( ) 
                         l a b e l s   =   l a b e l s . t o ( l m _ l o g i t s . d e v i c e ) 
                         m a s k e d _ l m _ l o s s   =   l o s s _ f c t ( l m _ l o g i t s . v i e w ( - 1 ,   s e l f . c o n f i g . v o c a b _ s i z e ) ,   l a b e l s . v i e w ( - 1 ) ) 
 
                 i f   n o t   r e t u r n _ d i c t : 
                         o u t p u t   =   ( l m _ l o g i t s , )   +   o u t p u t s [ 1 : ] 
                         r e t u r n   ( ( m a s k e d _ l m _ l o s s , )   +   o u t p u t )   i f   m a s k e d _ l m _ l o s s   i s   n o t   N o n e   e l s e   o u t p u t 
 
                 r e t u r n   S e a m l e s s M 4 T v 2 T e x t T o U n i t O u t p u t ( 
                         l a s t _ h i d d e n _ s t a t e = l m _ l o g i t s , 
                         p a d d i n g _ m a s k = o u t p u t s . p a d d i n g _ m a s k , 
                         d e c o d e r _ h i d d e n _ s t a t e s = o u t p u t s . d e c o d e r _ h i d d e n _ s t a t e s , 
                         d e c o d e r _ a t t e n t i o n s = o u t p u t s . d e c o d e r _ a t t e n t i o n s , 
                         e n c o d e r _ l a s t _ h i d d e n _ s t a t e = o u t p u t s . e n c o d e r _ l a s t _ h i d d e n _ s t a t e , 
                         e n c o d e r _ h i d d e n _ s t a t e s = o u t p u t s . e n c o d e r _ h i d d e n _ s t a t e s , 
                         e n c o d e r _ a t t e n t i o n s = o u t p u t s . e n c o d e r _ a t t e n t i o n s , 
                         l o s s = m a s k e d _ l m _ l o s s , 
                 ) 
 
         #   C o p i e d   f r o m   t r a n s f o r m e r s . m o d e l s . s e a m l e s s _ m 4 t . m o d e l i n g _ s e a m l e s s _ m 4 t . S e a m l e s s M 4 T T e x t T o U n i t F o r C o n d i t i o n a l G e n e r a t i o n . _ t i e _ w e i g h t s 
         d e f   _ t i e _ w e i g h t s ( s e l f )   - >   N o n e : 
                 i f   g e t a t t r ( s e l f . c o n f i g ,   " t i e _ w o r d _ e m b e d d i n g s " ,   T r u e ) : 
                         o u t p u t _ e m b e d d i n g s   =   s e l f . g e t _ o u t p u t _ e m b e d d i n g s ( ) 
                         i f   o u t p u t _ e m b e d d i n g s   i s   n o t   N o n e : 
                                 s e l f . _ t i e _ o r _ c l o n e _ w e i g h t s ( o u t p u t _ e m b e d d i n g s ,   s e l f . g e t _ i n p u t _ e m b e d d i n g s ( ) ) 
 
 
 # # # # # # # # # # # #   V O C O D E R   r e l a t e d   c o d e   # # # # # # # # # # # # # # # # 
 
 
 #   C o p i e d   f r o m   t r a n s f o r m e r s . m o d e l s . s p e e c h t 5 . m o d e l i n g _ s p e e c h t 5 . H i f i G a n R e s i d u a l B l o c k 
 c l a s s   H i f i G a n R e s i d u a l B l o c k ( n n . M o d u l e ) : 
         d e f   _ _ i n i t _ _ ( s e l f ,   c h a n n e l s ,   k e r n e l _ s i z e = 3 ,   d i l a t i o n = ( 1 ,   3 ,   5 ) ,   l e a k y _ r e l u _ s l o p e = 0 . 1 ) : 
                 s u p e r ( ) . _ _ i n i t _ _ ( ) 
                 s e l f . l e a k y _ r e l u _ s l o p e   =   l e a k y _ r e l u _ s l o p e 
 
                 s e l f . c o n v s 1   =   n n . M o d u l e L i s t ( 
                         [ 
                                 n n . C o n v 1 d ( 
                                         c h a n n e l s , 
                                         c h a n n e l s , 
                                         k e r n e l _ s i z e , 
                                         s t r i d e = 1 , 
                                         d i l a t i o n = d i l a t i o n [ i ] , 
                                         p a d d i n g = s e l f . g e t _ p a d d i n g ( k e r n e l _ s i z e ,   d i l a t i o n [ i ] ) , 
                                 ) 
                                 f o r   i   i n   r a n g e ( l e n ( d i l a t i o n ) ) 
                         ] 
                 ) 
                 s e l f . c o n v s 2   =   n n . M o d u l e L i s t ( 
                         [ 
                                 n n . C o n v 1 d ( 
                                         c h a n n e l s , 
                                         c h a n n e l s , 
                                         k e r n e l _ s i z e , 
                                         s t r i d e = 1 , 
                                         d i l a t i o n = 1 , 
                                         p a d d i n g = s e l f . g e t _ p a d d i n g ( k e r n e l _ s i z e ,   1 ) , 
                                 ) 
                                 f o r   _   i n   r a n g e ( l e n ( d i l a t i o n ) ) 
                         ] 
                 ) 
 
         d e f   g e t _ p a d d i n g ( s e l f ,   k e r n e l _ s i z e ,   d i l a t i o n = 1 ) : 
                 r e t u r n   ( k e r n e l _ s i z e   *   d i l a t i o n   -   d i l a t i o n )   / /   2 
 
         d e f   a p p l y _ w e i g h t _ n o r m ( s e l f ) : 
                 w e i g h t _ n o r m   =   n n . u t i l s . w e i g h t _ n o r m 
                 i f   h a s a t t r ( n n . u t i l s . p a r a m e t r i z a t i o n s ,   " w e i g h t _ n o r m " ) : 
                         w e i g h t _ n o r m   =   n n . u t i l s . p a r a m e t r i z a t i o n s . w e i g h t _ n o r m 
 
                 f o r   l a y e r   i n   s e l f . c o n v s 1 : 
                         w e i g h t _ n o r m ( l a y e r ) 
                 f o r   l a y e r   i n   s e l f . c o n v s 2 : 
                         w e i g h t _ n o r m ( l a y e r ) 
 
         d e f   r e m o v e _ w e i g h t _ n o r m ( s e l f ) : 
                 f o r   l a y e r   i n   s e l f . c o n v s 1 : 
                         n n . u t i l s . r e m o v e _ w e i g h t _ n o r m ( l a y e r ) 
                 f o r   l a y e r   i n   s e l f . c o n v s 2 : 
                         n n . u t i l s . r e m o v e _ w e i g h t _ n o r m ( l a y e r ) 
 
         d e f   f o r w a r d ( s e l f ,   h i d d e n _ s t a t e s ) : 
                 f o r   c o n v 1 ,   c o n v 2   i n   z i p ( s e l f . c o n v s 1 ,   s e l f . c o n v s 2 ) : 
                         r e s i d u a l   =   h i d d e n _ s t a t e s 
                         h i d d e n _ s t a t e s   =   n n . f u n c t i o n a l . l e a k y _ r e l u ( h i d d e n _ s t a t e s ,   s e l f . l e a k y _ r e l u _ s l o p e ) 
                         h i d d e n _ s t a t e s   =   c o n v 1 ( h i d d e n _ s t a t e s ) 
                         h i d d e n _ s t a t e s   =   n n . f u n c t i o n a l . l e a k y _ r e l u ( h i d d e n _ s t a t e s ,   s e l f . l e a k y _ r e l u _ s l o p e ) 
                         h i d d e n _ s t a t e s   =   c o n v 2 ( h i d d e n _ s t a t e s ) 
                         h i d d e n _ s t a t e s   =   h i d d e n _ s t a t e s   +   r e s i d u a l 
                 r e t u r n   h i d d e n _ s t a t e s 
 
 
 c l a s s   S e a m l e s s M 4 T v 2 V a r i a n c e P r e d i c t o r ( n n . M o d u l e ) : 
         d e f   _ _ i n i t _ _ ( s e l f ,   e m b e d _ d i m ,   h i d d e n _ d i m ,   k e r n e l _ s i z e ,   v a r _ p r e d _ d r o p o u t ) : 
                 s u p e r ( ) . _ _ i n i t _ _ ( ) 
 
                 s e l f . c o n v 1   =   n n . C o n v 1 d ( 
                         e m b e d _ d i m , 
                         h i d d e n _ d i m , 
                         k e r n e l _ s i z e = k e r n e l _ s i z e , 
                         p a d d i n g = " s a m e " , 
                 ) 
                 s e l f . a c t i v a t i o n _ f u c t i o n   =   n n . R e L U ( ) 
                 s e l f . l n 1   =   n n . L a y e r N o r m ( h i d d e n _ d i m ) 
                 s e l f . d r o p o u t _ m o d u l e   =   n n . D r o p o u t ( p = v a r _ p r e d _ d r o p o u t ) 
                 s e l f . c o n v 2   =   n n . C o n v 1 d ( 
                         h i d d e n _ d i m , 
                         h i d d e n _ d i m , 
                         k e r n e l _ s i z e = k e r n e l _ s i z e , 
                         p a d d i n g = " s a m e " , 
                 ) 
                 s e l f . l n 2   =   n n . L a y e r N o r m ( h i d d e n _ d i m ) 
                 s e l f . p r o j   =   n n . L i n e a r ( h i d d e n _ d i m ,   1 ) 
 
         d e f   f o r w a r d ( s e l f ,   h i d d e n _ s t a t e s :   T e n s o r ,   p a d d i n g _ m a s k :   O p t i o n a l [ T e n s o r ]   =   N o n e )   - >   T e n s o r : 
                 #   I n p u t :   B   x   T   x   C ;   O u t p u t :   B   x   T 
                 i f   p a d d i n g _ m a s k   i s   n o t   N o n e : 
                         h i d d e n _ s t a t e s   =   h i d d e n _ s t a t e s . m a s k e d _ f i l l ( ~ p a d d i n g _ m a s k . b o o l ( ) . u n s q u e e z e ( - 1 ) ,   0 . 0 ) 
                 h i d d e n _ s t a t e s   =   s e l f . c o n v 1 ( h i d d e n _ s t a t e s . t r a n s p o s e ( 1 ,   2 ) ) 
                 h i d d e n _ s t a t e s   =   s e l f . a c t i v a t i o n _ f u c t i o n ( h i d d e n _ s t a t e s ) . t r a n s p o s e ( 1 ,   2 ) 
                 h i d d e n _ s t a t e s   =   s e l f . d r o p o u t _ m o d u l e ( s e l f . l n 1 ( h i d d e n _ s t a t e s ) ) 
                 i f   p a d d i n g _ m a s k   i s   n o t   N o n e : 
                         h i d d e n _ s t a t e s   =   h i d d e n _ s t a t e s . m a s k e d _ f i l l ( ~ p a d d i n g _ m a s k . b o o l ( ) . u n s q u e e z e ( - 1 ) ,   0 . 0 ) 
                 h i d d e n _ s t a t e s   =   s e l f . c o n v 2 ( h i d d e n _ s t a t e s . t r a n s p o s e ( 1 ,   2 ) ) 
                 h i d d e n _ s t a t e s   =   s e l f . a c t i v a t i o n _ f u c t i o n ( h i d d e n _ s t a t e s ) . t r a n s p o s e ( 1 ,   2 ) 
                 h i d d e n _ s t a t e s   =   s e l f . d r o p o u t _ m o d u l e ( s e l f . l n 2 ( h i d d e n _ s t a t e s ) ) 
                 r e t u r n   s e l f . p r o j ( h i d d e n _ s t a t e s ) . s q u e e z e ( d i m = 2 ) 
 
 
 #   C o p i e d   f r o m   t r a n s f o r m e r s . m o d e l s . s e a m l e s s _ m 4 t . m o d e l i n g _ s e a m l e s s _ m 4 t . S e a m l e s s M 4 T H i f i G a n   w i t h   S e a m l e s s M 4 T - > S e a m l e s s M 4 T v 2 
 c l a s s   S e a m l e s s M 4 T v 2 H i f i G a n ( n n . M o d u l e ) : 
         d e f   _ _ i n i t _ _ ( s e l f ,   c o n f i g :   S e a m l e s s M 4 T v 2 C o n f i g ) : 
                 s u p e r ( ) . _ _ i n i t _ _ ( ) 
                 m o d e l _ i n _ d i m   =   c o n f i g . u n i t _ e m b e d _ d i m   +   c o n f i g . l a n g _ e m b e d _ d i m   +   c o n f i g . s p k r _ e m b e d _ d i m 
                 s e l f . l e a k y _ r e l u _ s l o p e   =   c o n f i g . l e a k y _ r e l u _ s l o p e 
                 s e l f . n u m _ k e r n e l s   =   l e n ( c o n f i g . r e s b l o c k _ k e r n e l _ s i z e s ) 
                 s e l f . n u m _ u p s a m p l e s   =   l e n ( c o n f i g . u p s a m p l e _ r a t e s ) 
                 s e l f . c o n v _ p r e   =   n n . C o n v 1 d ( 
                         m o d e l _ i n _ d i m , 
                         c o n f i g . u p s a m p l e _ i n i t i a l _ c h a n n e l , 
                         k e r n e l _ s i z e = 7 , 
                         s t r i d e = 1 , 
                         p a d d i n g = 3 , 
                 ) 
 
                 s e l f . u p s a m p l e r   =   n n . M o d u l e L i s t ( ) 
                 f o r   i ,   ( u p s a m p l e _ r a t e ,   k e r n e l _ s i z e )   i n   e n u m e r a t e ( z i p ( c o n f i g . u p s a m p l e _ r a t e s ,   c o n f i g . u p s a m p l e _ k e r n e l _ s i z e s ) ) : 
                         s e l f . u p s a m p l e r . a p p e n d ( 
                                 n n . C o n v T r a n s p o s e 1 d ( 
                                         c o n f i g . u p s a m p l e _ i n i t i a l _ c h a n n e l   / /   ( 2 * * i ) , 
                                         c o n f i g . u p s a m p l e _ i n i t i a l _ c h a n n e l   / /   ( 2   * *   ( i   +   1 ) ) , 
                                         k e r n e l _ s i z e = k e r n e l _ s i z e , 
                                         s t r i d e = u p s a m p l e _ r a t e , 
                                         p a d d i n g = ( k e r n e l _ s i z e   -   u p s a m p l e _ r a t e )   / /   2 , 
                                 ) 
                         ) 
 
                 s e l f . r e s b l o c k s   =   n n . M o d u l e L i s t ( ) 
                 f o r   i   i n   r a n g e ( l e n ( s e l f . u p s a m p l e r ) ) : 
                         c h a n n e l s   =   c o n f i g . u p s a m p l e _ i n i t i a l _ c h a n n e l   / /   ( 2   * *   ( i   +   1 ) ) 
                         f o r   k e r n e l _ s i z e ,   d i l a t i o n   i n   z i p ( c o n f i g . r e s b l o c k _ k e r n e l _ s i z e s ,   c o n f i g . r e s b l o c k _ d i l a t i o n _ s i z e s ) : 
                                 s e l f . r e s b l o c k s . a p p e n d ( H i f i G a n R e s i d u a l B l o c k ( c h a n n e l s ,   k e r n e l _ s i z e ,   d i l a t i o n ,   c o n f i g . l e a k y _ r e l u _ s l o p e ) ) 
 
                 s e l f . c o n v _ p o s t   =   n n . C o n v 1 d ( c h a n n e l s ,   1 ,   k e r n e l _ s i z e = 7 ,   s t r i d e = 1 ,   p a d d i n g = 3 ) 
 
         d e f   f o r w a r d ( s e l f ,   i n p u t _ e m b e d s :   t o r c h . F l o a t T e n s o r )   - >   t o r c h . F l o a t T e n s o r : 
                 r " " " 
                 C o n v e r t s   a   l o g - m e l   s p e c t r o g r a m   i n t o   a   s p e e c h   w a v e f o r m .   P a s s i n g   a   b a t c h   o f   l o g - m e l   s p e c t r o g r a m s   r e t u r n s   a   b a t c h 
                 o f   s p e e c h   w a v e f o r m s .   P a s s i n g   a   s i n g l e ,   u n - b a t c h e d   l o g - m e l   s p e c t r o g r a m   r e t u r n s   a   s i n g l e ,   u n - b a t c h e d   s p e e c h 
                 w a v e f o r m . 
 
                 A r g s : 
                         s p e c t r o g r a m   ( ` t o r c h . F l o a t T e n s o r ` ) : 
                                 T e n s o r   c o n t a i n i n g   t h e   l o g - m e l   s p e c t r o g r a m s .   C a n   b e   b a t c h e d   a n d   o f   s h a p e   ` ( b a t c h _ s i z e ,   s e q u e n c e _ l e n g t h , 
                                 m o d e l _ i n _ d i m ) ` ,   o r   u n - b a t c h e d   a n d   o f   s h a p e   ` ( s e q u e n c e _ l e n g t h ,   m o d e l _ i n _ d i m ) ` .   N o t e   t h a t   ` m o d e l _ i n _ d i m ` 
                                 i s   t h e   s u m   o f   ` c o n f i g . u n i t _ e m b e d _ d i m ` ,   ` c o n f i g . l a n g _ e m b e d _ d i m `   a n d   ` c o n f i g . s p k r _ e m b e d _ d i m ` . 
 
                 R e t u r n s : 
                         ` t o r c h . F l o a t T e n s o r ` :   T e n s o r   c o n t a i n i n g   t h e   s p e e c h   w a v e f o r m .   I f   t h e   i n p u t   s p e c t r o g r a m   i s   b a t c h e d ,   w i l l   b e   o f 
                         s h a p e   ` ( b a t c h _ s i z e ,   n u m _ f r a m e s , ) ` .   I f   u n - b a t c h e d ,   w i l l   b e   o f   s h a p e   ` ( n u m _ f r a m e s , ) ` . 
                 " " " 
 
                 h i d d e n _ s t a t e s   =   s e l f . c o n v _ p r e ( i n p u t _ e m b e d s ) 
                 f o r   i   i n   r a n g e ( s e l f . n u m _ u p s a m p l e s ) : 
                         h i d d e n _ s t a t e s   =   n n . f u n c t i o n a l . l e a k y _ r e l u ( h i d d e n _ s t a t e s ,   s e l f . l e a k y _ r e l u _ s l o p e ) 
                         h i d d e n _ s t a t e s   =   s e l f . u p s a m p l e r [ i ] ( h i d d e n _ s t a t e s ) 
 
                         r e s _ s t a t e   =   s e l f . r e s b l o c k s [ i   *   s e l f . n u m _ k e r n e l s ] ( h i d d e n _ s t a t e s ) 
                         f o r   j   i n   r a n g e ( 1 ,   s e l f . n u m _ k e r n e l s ) : 
                                 r e s _ s t a t e   + =   s e l f . r e s b l o c k s [ i   *   s e l f . n u m _ k e r n e l s   +   j ] ( h i d d e n _ s t a t e s ) 
                         h i d d e n _ s t a t e s   =   r e s _ s t a t e   /   s e l f . n u m _ k e r n e l s 
 
                 h i d d e n _ s t a t e s   =   n n . f u n c t i o n a l . l e a k y _ r e l u ( h i d d e n _ s t a t e s ) 
                 h i d d e n _ s t a t e s   =   s e l f . c o n v _ p o s t ( h i d d e n _ s t a t e s ) 
                 h i d d e n _ s t a t e s   =   t o r c h . t a n h ( h i d d e n _ s t a t e s ) 
 
                 #   r e m o v e   s e q - l e n   d i m   s i n c e   t h i s   c o l l a p s e s   t o   1 
                 w a v e f o r m   =   h i d d e n _ s t a t e s . s q u e e z e ( 1 ) 
 
                 r e t u r n   w a v e f o r m 
 
 
 @ a u t o _ d o c s t r i n g ( 
         c u s t o m _ i n t r o = " " " 
         C o d e   H i F i - G A N   v o c o d e r   a s   d e s c r i b e d   i n   t h i s   [ r e p o s i t o r y ] ( h t t p s : / / g i t h u b . c o m / f a c e b o o k r e s e a r c h / s p e e c h - r e s y n t h e s i s ) . 
         " " " 
 ) 
 c l a s s   S e a m l e s s M 4 T v 2 C o d e H i f i G a n ( P r e T r a i n e d M o d e l ) : 
         c o n f i g _ c l a s s   =   S e a m l e s s M 4 T v 2 C o n f i g 
         m a i n _ i n p u t _ n a m e   =   " i n p u t _ e m b e d s " 
         _ n o _ s p l i t _ m o d u l e s   =   [ ] 
 
         d e f   _ _ i n i t _ _ ( s e l f ,   c o n f i g ) : 
                 s u p e r ( ) . _ _ i n i t _ _ ( c o n f i g ) 
 
                 s e l f . p a d _ t o k e n _ i d   =   c o n f i g . t 2 u _ p a d _ t o k e n _ i d 
                 e m b e d _ d i m   =   c o n f i g . u n i t _ e m b e d _ d i m 
                 k e r n e l _ s i z e   =   c o n f i g . v a r i a n c e _ p r e d i c t o r _ k e r n e l _ s i z e 
                 v a r _ p r e d _ d r o p o u t   =   c o n f i g . v a r _ p r e d _ d r o p o u t 
                 s e l f . d u r _ p r e d i c t o r   =   S e a m l e s s M 4 T v 2 V a r i a n c e P r e d i c t o r ( e m b e d _ d i m ,   e m b e d _ d i m ,   k e r n e l _ s i z e ,   v a r _ p r e d _ d r o p o u t ) 
 
                 s e l f . u n i t _ e m b e d d i n g   =   n n . E m b e d d i n g ( c o n f i g . u n i t _ h i f i _ g a n _ v o c a b _ s i z e ,   c o n f i g . u n i t _ e m b e d _ d i m ) 
                 s e l f . s p e a k e r _ e m b e d d i n g   =   n n . E m b e d d i n g ( c o n f i g . v o c o d e r _ n u m _ s p k r s ,   c o n f i g . s p k r _ e m b e d _ d i m ) 
                 s e l f . l a n g u a g e _ e m b e d d i n g   =   n n . E m b e d d i n g ( c o n f i g . v o c o d e r _ n u m _ l a n g s ,   c o n f i g . l a n g _ e m b e d _ d i m ) 
 
                 s e l f . h i f i _ g a n   =   S e a m l e s s M 4 T v 2 H i f i G a n ( c o n f i g ) 
 
                 #   I n i t i a l i z e   w e i g h t s   a n d   a p p l y   f i n a l   p r o c e s s i n g 
                 s e l f . p o s t _ i n i t ( ) 
 
         #   C o p i e d   f r o m   t r a n s f o r m e r s . m o d e l s . s e a m l e s s _ m 4 t . m o d e l i n g _ s e a m l e s s _ m 4 t . S e a m l e s s M 4 T C o d e H i f i G a n . _ g e t _ d u r _ o u t p u t _ l e n g t h s 
         d e f   _ g e t _ d u r _ o u t p u t _ l e n g t h s ( s e l f ,   i n p u t _ i d s ,   d u r _ o u t ) : 
                 " " " 
                 C o m p u t e s   t h e   o u t p u t   l e n g t h   a f t e r   t h e   d u r a t i o n   l a y e r . 
                 " " " 
                 u n i t _ l e n g t h s   =   ( i n p u t _ i d s   ! =   s e l f . p a d _ t o k e n _ i d ) . s u m ( 1 ) 
 
                 #   t a k e   c a r e   o f   e d g e   c a s e s   w h e r e   n o   p a d d i n g   o r   t o o   m a n y   p a d d i n g 
                 u n i t _ l e n g t h s   =   t o r c h . c l a m p ( u n i t _ l e n g t h s ,   0 ,   d u r _ o u t . s h a p e [ 1 ]   -   1 ) 
 
                 c u m u l a t i v e _ d u r _ o u t   =   t o r c h . c u m s u m ( d u r _ o u t ,   d i m = 1 ) 
                 u n i t _ l e n g t h s   =   c u m u l a t i v e _ d u r _ o u t . g a t h e r ( d i m = 1 ,   i n d e x = u n i t _ l e n g t h s . u n s q u e e z e ( 1 ) ) . s q u e e z e ( ) 
 
                 r e t u r n   u n i t _ l e n g t h s 
 
         #   C o p i e d   f r o m   t r a n s f o r m e r s . m o d e l s . s e a m l e s s _ m 4 t . m o d e l i n g _ s e a m l e s s _ m 4 t . S e a m l e s s M 4 T C o d e H i f i G a n . _ g e t _ o u t p u t _ h i f i g a n _ l e n g t h s 
         d e f   _ g e t _ o u t p u t _ h i f i g a n _ l e n g t h s ( s e l f ,   i n p u t _ l e n g t h s :   U n i o n [ t o r c h . L o n g T e n s o r ,   i n t ] ) : 
                 " " " 
                 C o m p u t e s   t h e   o u t p u t   l e n g t h   o f   t h e   h i f i g a n   c o n v o l u t i o n a l   l a y e r s 
                 " " " 
 
                 d e f   _ c o n v _ o u t _ l e n g t h ( i n p u t _ l e n g t h ,   k e r n e l _ s i z e ,   s t r i d e ,   p a d ,   d i l a t i o n = 1 ) : 
                         #   1 D   c o n v o l u t i o n a l   l a y e r   o u t p u t   l e n g t h   f o r m u l a   t a k e n 
                         #   f r o m   h t t p s : / / p y t o r c h . o r g / d o c s / s t a b l e / g e n e r a t e d / t o r c h . n n . C o n v 1 d . h t m l 
                         r e t u r n   ( 
                                 t o r c h . d i v ( i n p u t _ l e n g t h   +   2   *   p a d   -   d i l a t i o n   *   ( k e r n e l _ s i z e   -   1 )   -   1 ,   s t r i d e ,   r o u n d i n g _ m o d e = " f l o o r " )   +   1 
                         ) 
 
                 d e f   _ t r a n s p o s e _ c o n v _ o u t _ l e n g t h ( i n p u t _ l e n g t h ,   k e r n e l _ s i z e ,   s t r i d e ,   p a d ,   d i l a t i o n = 1 ) : 
                         r e t u r n   ( i n p u t _ l e n g t h   -   1 )   *   s t r i d e   -   2   *   p a d   +   d i l a t i o n   *   ( k e r n e l _ s i z e   -   1 )   +   1 
 
       