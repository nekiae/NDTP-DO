import os
import json
import uuid
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

import chromadb
import numpy as np
from sentence_transformers import SentenceTransformer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModernRAGSystem:
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(
        self, 
        knowledge_base_path: str = "knowledge_base.json",
        model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        db_path: str = "./chroma_db"
    ):
        self.knowledge_base_path = Path(knowledge_base_path)
        self.db_path = db_path
        self.collection_name = "technopark_knowledge"
        self.last_indexed = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
        self.embedding_model = SentenceTransformer(model_name)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        
        # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        try:
            self.collection = self.chroma_client.get_collection(self.collection_name)
        except Exception:
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                metadata={"description": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–∞"}
            )
        
        self.knowledge_base = {}
        logger.info("‚úÖ –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def load_and_index_knowledge(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
            if self._need_reindexing():
                logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
                self._load_knowledge_base()
                self._create_embeddings()
                self.last_indexed = datetime.now()
                logger.info("‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            else:
                logger.info("üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É–∂–µ –∞–∫—Ç—É–∞–ª—å–Ω–∞")
                self._load_knowledge_base()
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            self.knowledge_base = {}
    
    def _need_reindexing(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        if not self.knowledge_base_path.exists():
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        count = self.collection.count()
        if count == 0:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
        file_mtime = datetime.fromtimestamp(self.knowledge_base_path.stat().st_mtime)
        if self.last_indexed is None or file_mtime > self.last_indexed:
            return True
        
        return False
    
    def _load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ JSON"""
        try:
            with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
            logger.info(f"üìñ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {list(self.knowledge_base.keys())}")
        except FileNotFoundError:
            logger.error(f"‚ùå –§–∞–π–ª {self.knowledge_base_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            self.knowledge_base = {}
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ JSON: {e}")
            self.knowledge_base = {}
    
    def _create_embeddings(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON"""
        if not self.knowledge_base:
            logger.warning("‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞")
            return

        # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        try:
            self.chroma_client.delete_collection(self.collection_name)
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                metadata={"description": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–∞"}
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é: {e}")

        documents = []
        metadatas = []
        ids = []

        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ
        about_info = self.knowledge_base.get("–æ_—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ", {})
        if about_info:
            doc_text = self._format_document(about_info, "about")
            documents.append(doc_text)
            metadatas.append({
                "section": "about",
                "type": "general_info",
                "title": "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ"
            })
            ids.append(str(uuid.uuid4()))

        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        directions = self.knowledge_base.get("–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è", [])
        for i, direction in enumerate(directions):
            doc_text = self._format_document(direction, "direction")
            documents.append(doc_text)
            metadatas.append({
                "section": "educational_directions",
                "type": "direction",
                "title": direction.get("–Ω–∞–∑–≤–∞–Ω–∏–µ", f"–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ {i+1}"),
                "direction_id": str(i)
            })
            ids.append(str(uuid.uuid4()))

        # 2.1. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π (–æ–±—â–∏–π –æ–±–∑–æ—Ä)
        directions_list = self.knowledge_base.get("—Å–ø–∏—Å–æ–∫_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π", [])
        if directions_list:
            doc_text = self._format_document({"—Å–ø–∏—Å–æ–∫": directions_list}, "directions_list")
            documents.append(doc_text)
            metadatas.append({
                "section": "directions_list",
                "type": "directions_overview",
                "title": "–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π"
            })
            ids.append(str(uuid.uuid4()))

        # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ—Ç–±–æ—Ä–∞
        selection_process = self.knowledge_base.get("–ø—Ä–æ—Ü–µ–¥—É—Ä–∞_–æ—Ç–±–æ—Ä–∞", {})
        if selection_process:
            doc_text = self._format_document(selection_process, "selection")
            documents.append(doc_text)
            metadatas.append({
                "section": "selection",
                "type": "selection_info",
                "title": "–ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ—Ç–±–æ—Ä–∞"
            })
            ids.append(str(uuid.uuid4()))

        # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è
        admission_info = self.knowledge_base.get("–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è_–¥–ª—è_–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è", "")
        if admission_info:
            doc_text = self._format_document({"description": admission_info}, "admission")
            documents.append(doc_text)
            metadatas.append({
                "section": "admission",
                "type": "admission_info",
                "title": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è"
            })
            ids.append(str(uuid.uuid4()))

        # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ–∫-–ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents_checklist = self.knowledge_base.get("documents_checklist", [])
        if documents_checklist:
            doc_text = self._format_document({"documents": documents_checklist}, "documents")
            documents.append(doc_text)
            metadatas.append({
                "section": "documents",
                "type": "documents_info",
                "title": "–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"
            })
            ids.append(str(uuid.uuid4()))

        # 6. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–º–µ–Ω
        sessions_info = self.knowledge_base.get("–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ_—Å–º–µ–Ω—ã", {})
        if sessions_info:
            doc_text = self._format_document(sessions_info, "sessions")
            documents.append(doc_text)
            metadatas.append({
                "section": "sessions",
                "type": "sessions_info",
                "title": "–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–º–µ–Ω—ã"
            })
            ids.append(str(uuid.uuid4()))

        # 7. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        remote_learning = self.knowledge_base.get("–¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ", {})
        if remote_learning:
            doc_text = self._format_document(remote_learning, "remote")
            documents.append(doc_text)
            metadatas.append({
                "section": "remote_learning",
                "type": "remote_info",
                "title": "–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
            })
            ids.append(str(uuid.uuid4()))

        # 8. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–∏
        accommodation = self.knowledge_base.get("–ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ", {})
        if accommodation:
            doc_text = self._format_document(accommodation, "accommodation")
            documents.append(doc_text)
            metadatas.append({
                "section": "accommodation",
                "type": "accommodation_info",
                "title": "–ü—Ä–æ–∂–∏–≤–∞–Ω–∏–µ –≤ –¢–µ—Ö–Ω–æ–¥–æ–º–µ"
            })
            ids.append(str(uuid.uuid4()))

        # 9. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø—Ä–æ–µ–∫—Ç—É
        project_requirements = self.knowledge_base.get("—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è_–∫_–ø—Ä–æ–µ–∫—Ç—É", {})
        if project_requirements:
            doc_text = self._format_document(project_requirements, "project_requirements")
            documents.append(doc_text)
            metadatas.append({
                "section": "project_requirements",
                "type": "project_info",
                "title": "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–µ–∫—Ç—É"
            })
            ids.append(str(uuid.uuid4()))

        # 10. –û–±—Ä–∞–±–æ—Ç–∫–∞ FAQ
        faq_items = self.knowledge_base.get("—á–∞—Å—Ç–æ_–∑–∞–¥–∞–≤–∞–µ–º—ã–µ_–≤–æ–ø—Ä–æ—Å—ã", [])
        for i, faq in enumerate(faq_items):
            doc_text = self._format_document(faq, "faq")
            documents.append(doc_text)
            metadatas.append({
                "section": "faq",
                "type": "faq_item",
                "title": faq.get("–≤–æ–ø—Ä–æ—Å", f"FAQ {i+1}"),
                "faq_id": str(i)
            })
            ids.append(str(uuid.uuid4()))

        # 11. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—å–≥–æ—Ç –¥–ª—è –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤
        benefits = self.knowledge_base.get("–ª—å–≥–æ—Ç—ã_–¥–ª—è_–≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤", {})
        if benefits:
            doc_text = self._format_document(benefits, "benefits")
            documents.append(doc_text)
            metadatas.append({
                "section": "benefits",
                "type": "benefits_info",
                "title": "–õ—å–≥–æ—Ç—ã –¥–ª—è –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤"
            })
            ids.append(str(uuid.uuid4()))

        if documents:
            logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings = self.embedding_model.encode(documents)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ ChromaDB
            self.collection.add(
                documents=documents,
                embeddings=embeddings.tolist(),
                metadatas=metadatas,
                ids=ids
            )

            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É")
        else:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    def _format_document(self, data: Dict[str, Any], doc_type: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥ –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON"""
        if doc_type == "about":
            parts = []
            if "–ø–æ–ª–Ω–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ" in data:
                parts.append(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {str(data['–ø–æ–ª–Ω–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ'])}")
            if "–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {str(data['–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            if "–º–∏—Å—Å–∏—è" in data:
                parts.append(f"–ú–∏—Å—Å–∏—è: {str(data['–º–∏—Å—Å–∏—è'])}")
            if "–∞–¥—Ä–µ—Å" in data:
                parts.append(f"–ê–¥—Ä–µ—Å: {str(data['–∞–¥—Ä–µ—Å'])}")
            if "–¥–∞—Ç–∞_–æ—Å–Ω–æ–≤–∞–Ω–∏—è" in data:
                parts.append(f"–î–∞—Ç–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏—è: {str(data['–¥–∞—Ç–∞_–æ—Å–Ω–æ–≤–∞–Ω–∏—è'])}")
            if "—Ü–µ–ª–µ–≤–∞—è_–∞—É–¥–∏—Ç–æ—Ä–∏—è" in data:
                parts.append(f"–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è: {str(data['—Ü–µ–ª–µ–≤–∞—è_–∞—É–¥–∏—Ç–æ—Ä–∏—è'])}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∏–Ω—Ü–∏–ø—ã
            principles = data.get("–ø—Ä–∏–Ω—Ü–∏–ø—ã", [])
            if principles and isinstance(principles, list):
                principles_str = ', '.join([str(p) for p in principles])
                parts.append(f"–ü—Ä–∏–Ω—Ü–∏–ø—ã: {principles_str}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
            infrastructure = data.get("–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", [])
            if infrastructure and isinstance(infrastructure, list):
                infra_str = ', '.join([str(i) for i in infrastructure])
                parts.append(f"–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {infra_str}")
            
            return " | ".join(parts)
        
        elif doc_type == "direction":
            parts = []
            if "–Ω–∞–∑–≤–∞–Ω–∏–µ" in data:
                parts.append(f"–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {str(data['–Ω–∞–∑–≤–∞–Ω–∏–µ'])}")
            if "–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {str(data['–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            
            return " | ".join(parts)
        
        elif doc_type == "directions_list":
            parts = []
            directions_list = data.get("—Å–ø–∏—Å–æ–∫", [])
            if directions_list and isinstance(directions_list, list):
                directions_str = ', '.join([str(direction) for direction in directions_list])
                parts.append(f"–í—Å–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ: {directions_str}")
                parts.append(f"–í—Å–µ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(directions_list)}")
            
            return " | ".join(parts)
        
        elif doc_type == "selection":
            parts = []
            if "–æ–±—â–µ–µ_–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {str(data['–æ–±—â–µ–µ_–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç—Ç–∞–ø—ã
            stage1 = data.get("—ç—Ç–∞–ø_1", {})
            if stage1:
                parts.append(f"–≠—Ç–∞–ø 1: {stage1.get('–Ω–∞–∑–≤–∞–Ω–∏–µ', '')} - {stage1.get('—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '')}")
            
            stage2 = data.get("—ç—Ç–∞–ø_2", {})
            if stage2:
                parts.append(f"–≠—Ç–∞–ø 2: {stage2.get('–Ω–∞–∑–≤–∞–Ω–∏–µ', '')} - {stage2.get('–æ–ø–∏—Å–∞–Ω–∏–µ', '')}")
            
            return " | ".join(parts)
        
        elif doc_type == "admission":
            parts = []
            if "description" in data:
                parts.append(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–∏: {str(data['description'])}")
            
            return " | ".join(parts)
        
        elif doc_type == "documents":
            parts = []
            documents_list = data.get("documents", [])
            if documents_list and isinstance(documents_list, list):
                docs_str = ', '.join([str(doc.get('name', '')) for doc in documents_list])
                parts.append(f"–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã: {docs_str}")
            
            return " | ".join(parts)
        
        elif doc_type == "sessions":
            parts = []
            if "–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–º–µ–Ω—ã: {str(data['–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            
            structure = data.get("—Å—Ç—Ä—É–∫—Ç—É—Ä–∞_—Å–º–µ–Ω—ã", [])
            if structure and isinstance(structure, list):
                struct_str = ', '.join([str(s) for s in structure])
                parts.append(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–º–µ–Ω—ã: {struct_str}")
            
            return " | ".join(parts)
        
        elif doc_type == "remote":
            parts = []
            if "–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {str(data['–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            
            features = data.get("–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏", [])
            if features and isinstance(features, list):
                features_str = ', '.join([str(f) for f in features])
                parts.append(f"–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: {features_str}")
            
            return " | ".join(parts)
        
        elif doc_type == "accommodation":
            parts = []
            if "–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–ü—Ä–æ–∂–∏–≤–∞–Ω–∏–µ: {str(data['–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            
            infrastructure = data.get("–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", [])
            if infrastructure and isinstance(infrastructure, list):
                infra_str = ', '.join([str(i) for i in infrastructure])
                parts.append(f"–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {infra_str}")
            
            return " | ".join(parts)
        
        elif doc_type == "project_requirements":
            parts = []
            if "–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–µ–∫—Ç—É: {str(data['–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            
            structure = data.get("—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", [])
            if structure and isinstance(structure, list):
                struct_str = ', '.join([str(s) for s in structure])
                parts.append(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞: {struct_str}")
            
            return " | ".join(parts)
        
        elif doc_type == "faq":
            parts = []
            if "–≤–æ–ø—Ä–æ—Å" in data:
                parts.append(f"–í–æ–ø—Ä–æ—Å: {str(data['–≤–æ–ø—Ä–æ—Å'])}")
            if "–æ—Ç–≤–µ—Ç" in data:
                parts.append(f"–û—Ç–≤–µ—Ç: {str(data['–æ—Ç–≤–µ—Ç'])}")
            
            return " | ".join(parts)
        
        elif doc_type == "benefits":
            parts = []
            if "–æ–ø–∏—Å–∞–Ω–∏–µ" in data:
                parts.append(f"–õ—å–≥–æ—Ç—ã –¥–ª—è –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤: {str(data['–æ–ø–∏—Å–∞–Ω–∏–µ'])}")
            
            return " | ".join(parts)
        
        # –û–±—â–∏–π —Å–ª—É—á–∞–π - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
        return str(data)
    
    async def search_async(
        self, 
        query: str, 
        max_results: int = 3,
        min_score: float = 0.3
    ) -> List[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self.search, query, max_results, min_score
        )
    
    def search(
        self, 
        query: str, 
        max_results: int = 3,
        min_score: float = 0.0
    ) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        logger.info(f"üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
        
        if self.collection.count() == 0:
            logger.warning("‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞")
            return []
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_model.encode([query])
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ ChromaDB
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=max_results * 2,  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                include=['documents', 'metadatas', 'distances']
            )
            
            if not results['documents'] or not results['documents'][0]:
                logger.info("ü§∑ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0], 
                results['distances'][0]
            )):
                # ChromaDB –∏—Å–ø–æ–ª—å–∑—É–µ—Ç squared euclidean distance, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ similarity
                # –ß–µ–º –º–µ–Ω—å—à–µ distance, —Ç–µ–º –±–æ–ª—å—à–µ similarity
                similarity = 1 / (1 + distance)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: distance 0 = similarity 1, –±–æ–ª—å—à–æ–π distance = similarity –±–ª–∏–∑–∫–∞ –∫ 0
                
                logger.info(f"üîç –î–æ–∫—É–º–µ–Ω—Ç: '{metadata.get('title')}' distance: {distance:.3f}, similarity: {similarity:.3f}")
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É score
                if similarity >= min_score:
                    processed_results.append({
                        "document": doc,
                        "metadata": metadata,
                        "similarity": similarity,
                        "section": metadata.get("section", "unknown"),
                        "title": metadata.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                        "type": metadata.get("type", "unknown")
                    })
                    
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: '{metadata.get('title')}' (similarity: {similarity:.3f})")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity
            processed_results.sort(key=lambda x: x['similarity'], reverse=True)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = processed_results[:max_results]
            logger.info(f"üìä –í–æ–∑–≤—Ä–∞—â–∞–µ–º {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {len(processed_results)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")
            
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}", exc_info=True)
            return []
    
    def get_context_for_query(self, query: str, max_context_length: int = None) -> str:
        """–£–º–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        logger.info(f"üîé –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è: '{query}'")
        
        query_lower = query.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º RAG
        greeting_words = ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å', '–Ω–∞—á–∞—Ç—å', '—Å—Ç–∞—Ä—Ç', 'hello', 'hi']
        if len(query_lower.split()) <= 2 and any(word in query_lower for word in greeting_words):
            logger.info("üëã –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç")
            return ""
        
        # –£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        query_expanded = self._expand_query(query_lower)
        logger.info(f"üîß –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query_expanded}'")
        
        # –©–ï–î–†–´–ô –ü–û–î–•–û–î: –¥–∞–¥–∏–º –ò–ò –º–∞–∫—Å–∏–º—É–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        logger.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º —â–µ–¥—Ä—ã–π –ø–æ–¥—Ö–æ–¥ - –º–∞–∫—Å–∏–º—É–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ò–ò")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        search_results = self.search(query_expanded, max_results=25, min_score=0.001)
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º, –ø–æ–ø—Ä–æ–±—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π
        if not search_results:
            logger.info("üîÑ –ü—Ä–æ–±—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è")
            search_results = self.search(query_lower, max_results=25, min_score=0.001)
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–∏—á–µ–≥–æ - –±–µ—Ä–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if not search_results:
            logger.info("üé≤ –ë–µ—Ä–µ–º —Ç–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
            search_results = self.search("—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫", max_results=20, min_score=0.0)
        
        if not search_results:
            logger.warning("‚ùå –ê–±—Å–æ–ª—é—Ç–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return ""
        
        # –£–ë–ò–†–ê–ï–ú –í–°–ï –ü–û–†–û–ì–ò - –±–µ—Ä–µ–º –í–°–ï —á—Ç–æ –Ω–∞—à–ª–∏
        context_parts = []
        total_length = 0
        used_results = 0
        
        for result in search_results:
            document = result["document"]
            title = result["title"]
            similarity = result["similarity"]
            
            # –ë–µ—Ä–µ–º –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            context_parts.append(document)
            total_length += len(document)
            used_results += 1
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω: '{title}' (similarity: {similarity:.3f}, {len(document)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        context = "\n\n".join(context_parts)
        logger.info(f"üìù –©–ï–î–†–´–ô –ö–û–ù–¢–ï–ö–°–¢: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {used_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        return context
    
    def _detect_query_intent(self, query: str) -> str:
        """–£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞
        intent_patterns = {
            'directions': ['–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º—ã', '–∫—É—Ä—Å—ã', '—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏', '–∏–∑—É—á–∞—é—Ç', '–æ–±—É—á–µ–Ω–∏–µ', '–∫–∏—Ç', '—Ä–æ–±–æ', '–±–∏–æ'],
            'location': ['–≥–¥–µ', '–∞–¥—Ä–µ—Å', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω', '–¥–æ–±—Ä–∞—Ç—å—Å—è', '—Ç–ø'],
            'admission': ['–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–ø–æ—Å—Ç—É–ø–∏—Ç—å', '–æ—Ç–±–æ—Ä', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–∫–∞–∫ –ø–æ–ø–∞—Å—Ç—å'],
            'accommodation': ['–ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '–æ–±—â–µ–∂–∏—Ç–∏–µ', '—Ç–µ—Ö–Ω–æ–¥–æ–º', '–∂–∏—Ç—å', '–Ω–æ—á–µ–≤–∞—Ç—å'],
            'schedule': ['—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–≤—Ä–µ–º—è', '–∫–æ–≥–¥–∞', '–≥—Ä–∞—Ñ–∏–∫', '—Å–º–µ–Ω—ã'],
            'contacts': ['–∫–æ–Ω—Ç–∞–∫—Ç—ã', '—Ç–µ–ª–µ—Ñ–æ–Ω', '—Å–≤—è–∑–∞—Ç—å—Å—è', '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '–ø–æ–º–æ—â—å'],
            'general': []  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        }
        
        for intent, keywords in intent_patterns.items():
            if any(keyword in query for keyword in keywords):
                return intent
        
        return 'general'
    
    def _get_search_parameters(self, intent: str) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        params = {
            'directions': {
                'max_results': 20,
                'min_score': 0.01,
                'relevance_threshold': 0.015,
                'min_filter': 0.01
            },
            'location': {
                'max_results': 10,
                'min_score': 0.01,
                'relevance_threshold': 0.02,
                'min_filter': 0.015
            },
            'admission': {
                'max_results': 15,
                'min_score': 0.02,
                'relevance_threshold': 0.025,
                'min_filter': 0.02
            },
            'accommodation': {
                'max_results': 12,
                'min_score': 0.02,
                'relevance_threshold': 0.025,
                'min_filter': 0.02
            },
            'general': {
                'max_results': 15,
                'min_score': 0.02,
                'relevance_threshold': 0.03,
                'min_filter': 0.02
            }
        }
        
        return params.get(intent, params['general'])
    
    def _expand_query(self, query: str) -> str:
        """–£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        expanded_parts = [query]
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –æ—á–µ–≤–∏–¥–Ω—ã–µ)
        common_abbreviations = {
            '–∫–∏—Ç': '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            '–∏—Ç': '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            '–≤—Ä': '–≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å',
            '–∞—Ä': '–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å',
            '—Ç–ø': '—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫'
        }
        
        query_lower = query.lower()
        for abbr, full in common_abbreviations.items():
            if abbr in query_lower:
                expanded_parts.append(full)
        
        # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        semantic_expansions = self._get_semantic_expansions(query_lower)
        expanded_parts.extend(semantic_expansions)
        
        # 3. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–∞
        contextual_terms = self._get_contextual_terms(query_lower)
        expanded_parts.extend(contextual_terms)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
        unique_parts = list(dict.fromkeys(expanded_parts))
        return " ".join(unique_parts)
    
    def _get_semantic_expansions(self, query: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        expansions = []
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        kb_terms = self._extract_key_terms()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∏ –Ω–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        query_words = set(query.lower().split())
        
        for category, terms in kb_terms.items():
            # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
            if any(word in terms for word in query_words):
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ç–µ—Ä–º–∏–Ω—ã —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                expansions.extend(terms)
                
        return list(set(expansions))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _extract_key_terms(self) -> Dict[str, List[str]]:
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        terms = {
            'location': [],
            'education': [],
            'admission': [],
            'accommodation': [],
            'general': []
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        if '–æ_—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ' in self.knowledge_base:
            about = self.knowledge_base['–æ_—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ']
            if '–∞–¥—Ä–µ—Å' in about:
                # –†–∞–∑–±–∏–≤–∞–µ–º –∞–¥—Ä–µ—Å –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
                address = about['–∞–¥—Ä–µ—Å'].lower()
                terms['location'].extend([
                    '—Ñ—Ä–∞–Ω—Ü–∏—Å–∫–∞ —Å–∫–æ—Ä–∏–Ω—ã', '–∞–¥—Ä–µ—Å', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫'
                ])
                # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏ –∞–¥—Ä–µ—Å–∞
                address_parts = address.replace(',', ' ').split()
                terms['location'].extend(address_parts)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        if '–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è' in self.knowledge_base:
            for direction in self.knowledge_base['–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è']:
                if '–Ω–∞–∑–≤–∞–Ω–∏–µ' in direction:
                    name = direction['–Ω–∞–∑–≤–∞–Ω–∏–µ'].lower()
                    terms['education'].extend(name.split())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        if '—Å–ø–∏—Å–æ–∫_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π' in self.knowledge_base:
            for direction in self.knowledge_base['—Å–ø–∏—Å–æ–∫_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π']:
                terms['education'].extend(direction.lower().split())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ—Ç–±–æ—Ä–∞
        if '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞_–æ—Ç–±–æ—Ä–∞' in self.knowledge_base:
            terms['admission'].extend(['–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–æ—Ç–±–æ—Ä', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è
        if '–ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ' in self.knowledge_base:
            terms['accommodation'].extend(['–ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '—Ç–µ—Ö–Ω–æ–¥–æ–º', '–æ–±—â–µ–∂–∏—Ç–∏–µ'])
        
        # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã
        for category in terms:
            # –£–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ –ø—Ä–µ–¥–ª–æ–≥–∏
            terms[category] = [
                term.strip().lower() 
                for term in terms[category] 
                if len(term.strip()) > 2 and term.strip() not in ['–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '—ç—Ç–æ', '–≤—Å–µ', '–∏–ª–∏', '–ø—Ä–∏']
            ]
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            terms[category] = list(set(terms[category]))
        
        return terms
    
    def _get_contextual_terms(self, query: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        contextual_terms = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
        query_patterns = {
            # –õ–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            'location': {
                'keywords': ['–≥–¥–µ', '–∞–¥—Ä–µ—Å', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω', '–¥–æ–±—Ä–∞—Ç—å—Å—è'],
                'context': ['—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫', '–∞–¥—Ä–µ—Å', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–∫–æ–Ω—Ç–∞–∫—Ç—ã']
            },
            # –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            'education': {
                'keywords': ['–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º—ã', '–∫—É—Ä—Å—ã', '—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏', '–∏–∑—É—á–∞—é—Ç', '–æ–±—É—á–µ–Ω–∏–µ'],
                'context': ['–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º—ã', '–æ–±—É—á–µ–Ω–∏–µ']
            },
            # –ü—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            'admission': {
                'keywords': ['–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–ø–æ—Å—Ç—É–ø–∏—Ç—å', '–æ—Ç–±–æ—Ä', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'],
                'context': ['–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–æ—Ç–±–æ—Ä', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è']
            },
            # –ü—Ä–æ–∂–∏–≤–∞–Ω–∏–µ
            'accommodation': {
                'keywords': ['–ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '–æ–±—â–µ–∂–∏—Ç–∏–µ', '—Ç–µ—Ö–Ω–æ–¥–æ–º', '–∂–∏—Ç—å'],
                'context': ['–ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '—Ç–µ—Ö–Ω–æ–¥–æ–º', '–∂–∏–ª—å–µ']
            },
            # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            'practical': {
                'keywords': ['–∫–∞–∫', '—á—Ç–æ', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ'],
                'context': ['–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø–æ–º–æ—â—å', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è']
            }
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        for pattern_type, pattern_data in query_patterns.items():
            if any(keyword in query for keyword in pattern_data['keywords']):
                contextual_terms.extend(pattern_data['context'])
                break  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
                
        return contextual_terms
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        return {
            "total_documents": self.collection.count(),
            "model_name": self.embedding_model.get_sentence_embedding_dimension(),
            "last_indexed": self.last_indexed.isoformat() if self.last_indexed else None,
            "knowledge_base_sections": list(self.knowledge_base.keys()),
            "collection_name": self.collection_name,
            "db_path": self.db_path
        }
    
    def reload_knowledge_base(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        logger.info("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        self.last_indexed = None
        self.load_and_index_knowledge()


# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
modern_rag_instance = None

def set_global_instance(instance):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏"""
    global modern_rag_instance
    modern_rag_instance = instance

async def get_context_for_query_async(query: str) -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    global modern_rag_instance
    if modern_rag_instance is None:
        return ""
    return modern_rag_instance.get_context_for_query(query)

def get_context_for_query(query: str) -> str:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    global modern_rag_instance
    if modern_rag_instance is None:
        return ""
    return modern_rag_instance.get_context_for_query(query) 