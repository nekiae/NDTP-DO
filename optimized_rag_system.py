import json
import hashlib
import logging
import time
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass, field
from collections import Counter

try:
    import chromadb
    import numpy as np
    from sentence_transformers import SentenceTransformer
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CachedResult:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
    context: str
    timestamp: datetime
    usage_count: int = 0
    query_hash: str = ""
    semantic_key: str = ""
    tokens_saved: int = 0
    original_tokens: int = 0

@dataclass
class QueryIntent:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
    intent_type: str
    confidence: float
    keywords: List[str] = field(default_factory=list)
    entities: List[str] = field(default_factory=list)

class SmartContextOptimizer:
    """üß† –£–º–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏–µ–π"""
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –±–ª–æ–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–≤—ã—à–µ = –≤–∞–∂–Ω–µ–µ)
    CONTENT_PRIORITIES = {
        '–∫–æ–Ω—Ç–∞–∫—Ç—ã': 10,
        '–∞–¥—Ä–µ—Å': 10,
        '—Ç–µ–ª–µ—Ñ–æ–Ω': 10,
        '—Ü–µ–Ω–∞': 9,
        '—Å—Ç–æ–∏–º–æ—Å—Ç—å': 9,
        '–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ': 8,
        '–¥–æ–∫—É–º–µ–Ω—Ç—ã': 8,
        '–æ—Ç–±–æ—Ä': 8,
        '–ø—Ä–æ–≥—Ä–∞–º–º—ã': 7,
        '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è': 7,
        '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ': 6,
        '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è': 5,
        '–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è': 4
    }
    
    # –°–æ–∫—Ä–∞—â–µ–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
    ABBREVIATIONS = {
        '–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –¥–µ—Ç—Å–∫–∏–π —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫': '–ù–î–¢',
        '–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã': '–ø—Ä–æ–≥—Ä–∞–º–º—ã',
        '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': '–ò–¢',
        '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç': '–ò–ò',
        '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è': '–¥–æ–ø. –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è',
        '–æ–±—É—á–∞—é—â–∏–µ—Å—è': '—É—á–∞—â–∏–µ—Å—è',
        '–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å': '–æ–±—É—á–µ–Ω–∏–µ',
        '–∑–∞—è–≤–ª–µ–Ω–∏–µ': '–∑–∞—è–≤–∫–∞',
        '–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç': '–¥–æ–∫—É–º–µ–Ω—Ç—ã'
    }
    
    def optimize_context(self, context: str, query: str, max_tokens: int) -> str:
        """üî• –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not context:
            return context
            
        original_tokens = len(context) // 3
        if original_tokens <= max_tokens:
            return context
        
        # 1. –ü—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è –±–ª–æ–∫–æ–≤
        prioritized_context = self._prioritize_blocks(context, query)
        
        # 2. –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ñ—Ä–∞–∑
        shortened_context = self._apply_abbreviations(prioritized_context)
        
        # 3. –£–º–Ω–æ–µ —É—Ä–µ–∑–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        optimized_context = self._smart_sentence_trimming(shortened_context, query)
        
        # 4. –§–∏–Ω–∞–ª—å–Ω–æ–µ —É—Ä–µ–∑–∞–Ω–∏–µ –¥–æ –ª–∏–º–∏—Ç–∞
        final_context = self._final_trim(optimized_context, max_tokens)
        
        final_tokens = len(final_context) // 3
        logger.info(f"üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: {original_tokens} ‚Üí {final_tokens} —Ç–æ–∫–µ–Ω–æ–≤ ({((original_tokens-final_tokens)/original_tokens*100):.1f}% —ç–∫–æ–Ω–æ–º–∏—è)")
        
        return final_context
    
    def _prioritize_blocks(self, context: str, query: str) -> str:
        """–ü—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è –±–ª–æ–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        blocks = context.split('\n\n')
        scored_blocks = []
        
        for block in blocks:
            if not block.strip():
                continue
                
            score = 0
            block_lower = block.lower()
            
            # –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            for keyword, priority in self.CONTENT_PRIORITIES.items():
                if keyword in block_lower:
                    score += priority
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–ø—Ä–æ—Å—É
            query_words = set(query_lower.split())
            block_words = set(block_lower.split())
            relevance = len(query_words & block_words) / len(query_words) if query_words else 0
            score += relevance * 15
            
            scored_blocks.append((score, block))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø –±–ª–æ–∫–∏
        scored_blocks.sort(reverse=True, key=lambda x: x[0])
        return '\n\n'.join([block for _, block in scored_blocks[:6]])  # –¢–æ–ø 6 –±–ª–æ–∫–æ–≤
    
    def _apply_abbreviations(self, text: str) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è"""
        for full_form, abbrev in self.ABBREVIATIONS.items():
            text = text.replace(full_form, abbrev)
        return text
    
    def _smart_sentence_trimming(self, text: str, query: str) -> str:
        """–£–º–Ω–æ–µ —É—Ä–µ–∑–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        sentences = text.split('. ')
        query_words = set(query.lower().split())
        
        scored_sentences = []
        for sentence in sentences:
            if len(sentence.strip()) < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ
                continue
                
            sentence_words = set(sentence.lower().split())
            relevance = len(query_words & sentence_words) / len(query_words) if query_words else 0.1
            
            # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
            if any(key in sentence.lower() for key in ['–∫–æ–Ω—Ç–∞–∫—Ç', '–∞–¥—Ä–µ—Å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å']):
                relevance += 0.5
            
            scored_sentences.append((relevance, sentence))
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        scored_sentences.sort(reverse=True, key=lambda x: x[0])
        selected = [sent for _, sent in scored_sentences[:12]]  # –¢–æ–ø 12 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        
        return '. '.join(selected) + '.'
    
    def _final_trim(self, text: str, max_tokens: int) -> str:
        """–§–∏–Ω–∞–ª—å–Ω–æ–µ —É—Ä–µ–∑–∞–Ω–∏–µ –¥–æ –ª–∏–º–∏—Ç–∞"""
        target_chars = max_tokens * 3
        if len(text) <= target_chars:
            return text
        
        # –ò—â–µ–º —Ö–æ—Ä–æ—à–µ–µ –º–µ—Å—Ç–æ –¥–ª—è –æ–±—Ä–µ–∑–∫–∏
        sentences = text[:target_chars].split('. ')
        if len(sentences) > 1:
            return '. '.join(sentences[:-1]) + '.'
        else:
            return text[:target_chars] + '...'

class AdvancedRAGCache:
    """üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self, max_size: int = 1000):
        self.exact_cache: Dict[str, CachedResult] = {}  # –¢–æ—á–Ω—ã–π –∫—ç—à
        self.semantic_cache: Dict[str, List[CachedResult]] = {}  # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à
        self.popular_cache: Dict[str, CachedResult] = {}  # –ö—ç—à –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.query_patterns: Counter = Counter()  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        self.max_size = max_size
        
        # –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        self.popular_patterns = {
            '–∞–¥—Ä–µ—Å': "üìç –ù–î–¢, –ú–∏–Ω—Å–∫\nüë• –î–ª—è —É—á–∞—â–∏—Ö—Å—è 9-11 –∫–ª–∞—Å—Å–æ–≤",
            '–∫–æ–Ω—Ç–∞–∫—Ç—ã': "üìû –¢–µ–ª–µ—Ñ–æ–Ω: —É–∫–∞–∑–∞–Ω –Ω–∞ —Å–∞–π—Ç–µ\nüìß Email: –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ",
            '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è': "üéì 15 –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π:\n‚Ä¢ –ò–¢\n‚Ä¢ –†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞\n‚Ä¢ –ë–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏\n‚Ä¢ –ò –¥—Ä—É–≥–∏–µ",
            '–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ': "üìù –û—Ç–±–æ—Ä: 2 —ç—Ç–∞–ø–∞\n1Ô∏è‚É£ –ó–∞–æ—á–Ω—ã–π: –∑–∞—è–≤–∫–∞ + –ø—Ä–æ–µ–∫—Ç\n2Ô∏è‚É£ –û—á–Ω—ã–π: —Ç–µ—Å—Ç + —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ"
        }
    
    def get_cached_result(self, query: str) -> Optional[CachedResult]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ (–º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫)"""
        query_hash = self._hash_query(query)
        
        # 1. –¢–æ—á–Ω—ã–π –∫—ç—à
        if query_hash in self.exact_cache:
            result = self.exact_cache[query_hash]
            if self._is_cache_valid(result):
                result.usage_count += 1
                logger.info(f"‚ö° –¢–æ—á–Ω—ã–π –∫—ç—à —Ö–∏—Ç: {query[:30]}...")
                return result
        
        # 2. –ü–æ–ø—É–ª—è—Ä–Ω—ã–π –∫—ç—à
        normalized_query = self._normalize_query(query)
        if normalized_query in self.popular_cache:
            result = self.popular_cache[normalized_query]
            result.usage_count += 1
            logger.info(f"üî• –ü–æ–ø—É–ª—è—Ä–Ω—ã–π –∫—ç—à —Ö–∏—Ç: {query[:30]}...")
            return result
        
        # 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à
        semantic_key = self._get_semantic_key(query)
        if semantic_key in self.semantic_cache:
            for cached_result in self.semantic_cache[semantic_key]:
                if self._is_semantically_similar(query, cached_result.query_hash) and self._is_cache_valid(cached_result):
                    cached_result.usage_count += 1
                    logger.info(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à —Ö–∏—Ç: {query[:30]}...")
                    return cached_result
        
        return None
    
    def cache_result(self, query: str, context: str, original_tokens: int = 0):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à"""
        query_hash = self._hash_query(query)
        semantic_key = self._get_semantic_key(query)
        
        result = CachedResult(
            context=context,
            timestamp=datetime.now(),
            query_hash=query_hash,
            semantic_key=semantic_key,
            original_tokens=original_tokens,
            tokens_saved=max(0, original_tokens - len(context) // 3)
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ç–æ—á–Ω—ã–π –∫—ç—à
        self.exact_cache[query_hash] = result
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à
        if semantic_key not in self.semantic_cache:
            self.semantic_cache[semantic_key] = []
        self.semantic_cache[semantic_key].append(result)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        normalized = self._normalize_query(query)
        self.query_patterns[normalized] += 1
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å—Ç–∞–ª –ø–æ–ø—É–ª—è—Ä–Ω—ã–º - –¥–æ–±–∞–≤–ª—è–µ–º –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∫—ç—à
        if self.query_patterns[normalized] >= 3:
            self.popular_cache[normalized] = result
            logger.info(f"üìà –ó–∞–ø—Ä–æ—Å –¥–æ–±–∞–≤–ª–µ–Ω –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∫—ç—à: {normalized}")
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
        if len(self.exact_cache) > self.max_size:
            self._cleanup_cache()
    
    def _hash_query(self, query: str) -> str:
        """–•—ç—à –∑–∞–ø—Ä–æ—Å–∞"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()
    
    def _normalize_query(self, query: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        query = query.lower().strip()
        # –ó–∞–º–µ–Ω—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
        synonyms = {
            '–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è': '–∞–¥—Ä–µ—Å',
            '–∫–∞–∫ –¥–æ–±—Ä–∞—Ç—å—Å—è': '–∞–¥—Ä–µ—Å',
            '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ': '–∞–¥—Ä–µ—Å',
            '—Å–≤—è–∑–∞—Ç—å—Å—è': '–∫–æ–Ω—Ç–∞–∫—Ç—ã',
            '—Ç–µ–ª–µ—Ñ–æ–Ω': '–∫–æ–Ω—Ç–∞–∫—Ç—ã',
            '–∫—É—Ä—Å—ã': '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è',
            '–ø—Ä–æ–≥—Ä–∞–º–º—ã': '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è',
            '–∫–∞–∫ –ø–æ—Å—Ç—É–ø–∏—Ç—å': '–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ',
            '–¥–æ–∫—É–º–µ–Ω—Ç—ã': '–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ'
        }
        
        for synonym, canonical in synonyms.items():
            if synonym in query:
                return canonical
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        for keyword in ['–∞–¥—Ä–µ—Å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å']:
            if keyword in query:
                return keyword
        
        return query[:20]  # –ü–µ—Ä–≤—ã–µ 20 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ fallback
    
    def _get_semantic_key(self, query: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–ª—é—á –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keywords = {
            'location': ['–∞–¥—Ä–µ—Å', '–≥–¥–µ', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–¥–æ–±—Ä–∞—Ç—å—Å—è'],
            'contact': ['–∫–æ–Ω—Ç–∞–∫—Ç', '—Ç–µ–ª–µ—Ñ–æ–Ω', '—Å–≤—è–∑—å', '–Ω–∞–ø–∏—Å–∞—Ç—å'],
            'programs': ['–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º—ã', '–∫—É—Ä—Å—ã', '–∏–∑—É—á–∞—é—Ç', '—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏'],
            'admission': ['–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–ø–æ—Å—Ç—É–ø–∏—Ç—å', '–æ—Ç–±–æ—Ä', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'],
            'cost': ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø–ª–∞—Ç–∏—Ç—å', '–¥–µ–Ω—å–≥–∏', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ'],
            'schedule': ['—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–≥—Ä–∞—Ñ–∏–∫', '–≤—Ä–µ–º—è', '—Å–º–µ–Ω—ã'],
            'general': ['–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–¥–∞–Ω–Ω—ã–µ', '—Å–≤–µ–¥–µ–Ω–∏—è', '–ø—Ä–æ', '–æ']
        }
        
        query_lower = query.lower()
        for key, words in keywords.items():
            if any(word in query_lower for word in words):
                return key
        
        return 'general'
    
    def _is_semantically_similar(self, query1: str, query2_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        return True  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    
    def _is_cache_valid(self, result: CachedResult) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫—ç—à–∞"""
        # –ö—ç—à –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω 24 —á–∞—Å–∞ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if result.usage_count >= 5:
            return (datetime.now() - result.timestamp) < timedelta(hours=24)
        # 12 —á–∞—Å–æ–≤ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        return (datetime.now() - result.timestamp) < timedelta(hours=12)
    
    def _cleanup_cache(self):
        """–£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞...")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∏ —Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∑–∞–ø–∏—Å–∏
        cutoff_time = datetime.now() - timedelta(hours=6)
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–æ—á–Ω–æ–≥–æ –∫—ç—à–∞
        to_remove = []
        for key, result in self.exact_cache.items():
            if result.timestamp < cutoff_time and result.usage_count < 2:
                to_remove.append(key)
        
        for key in to_remove:
            del self.exact_cache[key]
        
        # –û—á–∏—Å—Ç–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫—ç—à–∞
        for key in list(self.semantic_cache.keys()):
            self.semantic_cache[key] = [
                result for result in self.semantic_cache[key]
                if result.timestamp >= cutoff_time or result.usage_count >= 2
            ]
            if not self.semantic_cache[key]:
                del self.semantic_cache[key]
        
        logger.info(f"‚úÖ –ö—ç—à –æ—á–∏—â–µ–Ω. –£–¥–∞–ª–µ–Ω–æ: {len(to_remove)} –∑–∞–ø–∏—Å–µ–π")
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        total_usage = sum(result.usage_count for result in self.exact_cache.values())
        total_tokens_saved = sum(result.tokens_saved for result in self.exact_cache.values())
        
        return {
            "exact_cache_size": len(self.exact_cache),
            "semantic_cache_groups": len(self.semantic_cache),
            "popular_cache_size": len(self.popular_cache),
            "total_usage_count": total_usage,
            "total_tokens_saved": total_tokens_saved,
            "top_patterns": dict(self.query_patterns.most_common(5))
        }

class TokenOptimizedRAG:
    """üöÄ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self, knowledge_base_path: str = "knowledge_base.json", max_tokens: int = 500):
        self.knowledge_base_path = Path(knowledge_base_path)
        self.max_tokens = max_tokens
        self.knowledge_base = {}
        self.cache = AdvancedRAGCache()
        self.optimizer = SmartContextOptimizer()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'tokens_saved': 0,
            'processing_time_saved': 0.0
        }
        
        self.load_knowledge()
        self._preload_popular_queries()
        
        logger.info(f"üéØ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è RAG –≥–æ—Ç–æ–≤–∞ | –õ–∏–º–∏—Ç: {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    
    def _preload_popular_queries(self):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        for pattern, response in self.cache.popular_patterns.items():
            self.cache.popular_cache[pattern] = CachedResult(
                context=response,
                timestamp=datetime.now(),
                usage_count=10,
                query_hash=self.cache._hash_query(pattern)
            )
        logger.info("üìö –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
    
    def load_knowledge(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
            logger.info("üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            self.knowledge_base = {}
    
    def get_context(self, query: str) -> str:
        """üéØ –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        start_time = time.time()
        self.stats['total_queries'] += 1
        
        # 1. –ü–†–û–í–ï–†–Ø–ï–ú –ú–ù–û–ì–û–£–†–û–í–ù–ï–í–´–ô –ö–≠–®
        cached_result = self.cache.get_cached_result(query)
        if cached_result:
            self.stats['cache_hits'] += 1
            self.stats['processing_time_saved'] += 0.5  # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç–∫–æ–Ω–æ–º–∏–∏
            return cached_result.context
        
        # 2. –ë–´–°–¢–†–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ü–û –¢–ò–ü–£ –ó–ê–ü–†–û–°–ê
        intent = self._analyze_query_intent(query)
        context = self._get_context_by_intent(query, intent)
        original_tokens = len(context) // 3
        
        # 3. –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
        optimized_context = self.optimizer.optimize_context(context, query, self.max_tokens)
        
        # 4. –ö–≠–®–ò–†–£–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢
        self.cache.cache_result(query, optimized_context, original_tokens)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        tokens_saved = original_tokens - len(optimized_context) // 3
        self.stats['tokens_saved'] += tokens_saved
        processing_time = time.time() - start_time
        
        logger.info(f"üî• –û–±—Ä–∞–±–æ—Ç–∞–Ω: {query[:30]}... | –¢–æ–∫–µ–Ω–æ–≤: {len(optimized_context)//3} | –í—Ä–µ–º—è: {processing_time:.3f}—Å")
        
        return optimized_context
    
    def _analyze_query_intent(self, query: str) -> QueryIntent:
        """üß† –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower().strip()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        intent_patterns = {
            'greeting': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π', 'hello', 'hi', '–Ω–∞—á–∞—Ç—å', '—Å—Ç–∞—Ä—Ç'],
            'location': ['–∞–¥—Ä–µ—Å', '–≥–¥–µ', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–¥–æ–±—Ä–∞—Ç—å—Å—è', '–≥–µ–æ–≥—Ä–∞—Ñ–∏—è'],
            'contact': ['–∫–æ–Ω—Ç–∞–∫—Ç', '—Ç–µ–ª–µ—Ñ–æ–Ω', '—Å–≤—è–∑—å', '–Ω–∞–ø–∏—Å–∞—Ç—å', '–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è'],
            'directions': ['–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º—ã', '–∫—É—Ä—Å—ã', '—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏', '–∏–∑—É—á–∞—é—Ç', '—Å–ø–∏—Å–æ–∫'],
            'admission': ['–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–ø–æ—Å—Ç—É–ø–∏—Ç—å', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '–æ—Ç–±–æ—Ä', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–∫–∞–∫ –ø–æ–ø–∞—Å—Ç—å'],
            'cost': ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø–ª–∞—Ç–∏—Ç—å', '–¥–µ–Ω—å–≥–∏', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–∑–∞—Ç—Ä–∞—Ç—ã'],
            'schedule': ['—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–≥—Ä–∞—Ñ–∏–∫', '–≤—Ä–µ–º—è', '—Å–º–µ–Ω—ã', '–∫–æ–≥–¥–∞'],
            'general': ['–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–¥–∞–Ω–Ω—ã–µ', '—Å–≤–µ–¥–µ–Ω–∏—è', '–ø—Ä–æ', '–æ', '—Ä–∞—Å—Å–∫–∞–∂–∏']
        }
        
        for intent_type, keywords in intent_patterns.items():
            matches = sum(1 for keyword in keywords if keyword in query_lower)
            if matches > 0:
                confidence = min(1.0, matches / 3.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                return QueryIntent(
                    intent_type=intent_type,
                    confidence=confidence,
                    keywords=[kw for kw in keywords if kw in query_lower]
                )
        
        return QueryIntent(intent_type='general', confidence=0.5)
    
    def _get_context_by_intent(self, query: str, intent: QueryIntent) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Ç–∏–ø—É –Ω–∞–º–µ—Ä–µ–Ω–∏—è"""
        
        if intent.intent_type == 'greeting':
            return ""  # –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π
        
        elif intent.intent_type == 'location':
            return self._get_location_info()
        
        elif intent.intent_type == 'contact':
            return self._get_contact_info()
        
        elif intent.intent_type == 'directions':
            return self._get_directions_info()
        
        elif intent.intent_type == 'admission':
            return self._get_admission_info()
        
        elif intent.intent_type == 'cost':
            return self._get_cost_info()
        
        elif intent.intent_type == 'schedule':
            return self._get_schedule_info()
        
        else:
            return self._get_general_info()
    
    def _get_location_info(self) -> str:
        """–ö—Ä–∞—Ç–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–∏"""
        about = self.knowledge_base.get("–æ_—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ", {})
        if about.get('–∞–¥—Ä–µ—Å'):
            contact = about.get('–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', {})
            result = f"üìç {about['–∞–¥—Ä–µ—Å']}"
            if contact.get('—Ç–µ–ª–µ—Ñ–æ–Ω'):
                result += f"\nüìû {contact['—Ç–µ–ª–µ—Ñ–æ–Ω']}"
            if about.get('—Ü–µ–ª–µ–≤–∞—è_–∞—É–¥–∏—Ç–æ—Ä–∏—è'):
                result += f"\nüë• {about['—Ü–µ–ª–µ–≤–∞—è_–∞—É–¥–∏—Ç–æ—Ä–∏—è']}"
            return result
        return "üìç –ù–î–¢, –ú–∏–Ω—Å–∫\nüë• –î–ª—è —É—á–∞—â–∏—Ö—Å—è 9-11 –∫–ª–∞—Å—Å–æ–≤"
    
    def _get_contact_info(self) -> str:
        """–ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        about = self.knowledge_base.get("–æ_—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ", {})
        contact = about.get('–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', {})
        
        result = "üìû –ö–æ–Ω—Ç–∞–∫—Ç—ã –ù–î–¢:\n"
        if contact.get('—Ç–µ–ª–µ—Ñ–æ–Ω'):
            result += f"‚Ä¢ –¢–µ–ª–µ—Ñ–æ–Ω: {contact['—Ç–µ–ª–µ—Ñ–æ–Ω']}\n"
        if contact.get('email'):
            result += f"‚Ä¢ Email: {contact['email']}\n"
        if contact.get('—Å–∞–π—Ç'):
            result += f"‚Ä¢ –°–∞–π—Ç: {contact['—Å–∞–π—Ç']}"
        
        return result if len(result) > 20 else "üìû –ö–æ–Ω—Ç–∞–∫—Ç—ã —É–∫–∞–∑–∞–Ω—ã –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ"
    
    def _get_directions_info(self) -> str:
        """–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        directions = self.knowledge_base.get("—Å–ø–∏—Å–æ–∫_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π", [])
        if directions:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
            top_directions = directions[:10]
            return "üéì –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:\n" + "\n".join(f"‚Ä¢ {d}" for d in top_directions)
        
        # Fallback
        return "üéì 15 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π: –ò–¢, —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞, –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –º–µ—Ö–∞—Ç—Ä–æ–Ω–∏–∫–∞ –∏ –¥—Ä."
    
    def _get_admission_info(self) -> str:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–∏"""
        selection = self.knowledge_base.get("–ø—Ä–æ—Ü–µ–¥—É—Ä–∞_–æ—Ç–±–æ—Ä–∞", {})
        if selection:
            stages = selection.get("—ç—Ç–∞–ø—ã", [])
            if stages:
                result = "üìù –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ—Ç–±–æ—Ä–∞:\n"
                for i, stage in enumerate(stages[:2], 1):  # –ü–µ—Ä–≤—ã–µ 2 —ç—Ç–∞–ø–∞
                    result += f"{i}Ô∏è‚É£ {stage.get('–Ω–∞–∑–≤–∞–Ω–∏–µ', f'–≠—Ç–∞–ø {i}')}: {stage.get('–æ–ø–∏—Å–∞–Ω–∏–µ', '–¥–µ—Ç–∞–ª–∏ –Ω–∞ —Å–∞–π—Ç–µ')}\n"
                return result
        
        return "üìù –û—Ç–±–æ—Ä: 2 —ç—Ç–∞–ø–∞\n1Ô∏è‚É£ –ó–∞–æ—á–Ω—ã–π: –∑–∞—è–≤–∫–∞ + –ø—Ä–æ–µ–∫—Ç\n2Ô∏è‚É£ –û—á–Ω—ã–π: —Ç–µ—Å—Ç + —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ"
    
    def _get_cost_info(self) -> str:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
        about = self.knowledge_base.get("–æ_—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ", {})
        if '–±–µ—Å–ø–ª–∞—Ç–Ω–æ' in str(about).lower():
            return "ÔøΩÔøΩ –û–±—É—á–µ–Ω–∏–µ –ë–ï–°–ü–õ–ê–¢–ù–û–ï\nüéØ –§–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç—Å—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ–º"
        return "üí∞ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —É—Ç–æ—á–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–∏"
    
    def _get_schedule_info(self) -> str:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–∏"""
        sessions = self.knowledge_base.get("–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ_—Å–º–µ–Ω—ã", {})
        if sessions:
            return "üìÖ –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–º–µ–Ω—ã –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ\nüìã –ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å–∞–π—Ç–µ"
        return "üìÖ –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ —Å–º–µ–Ω –∏ –∑–∞–Ω—è—Ç–∏–π —Ä–∞–∑–º–µ—â–∞–µ—Ç—Å—è –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ"
    
    def _get_general_info(self) -> str:
        """–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        about = self.knowledge_base.get("–æ_—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ", {})
        if about:
            result = f"üè´ {about.get('–ø–æ–ª–Ω–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ', '–ù–î–¢')}\n"
            if about.get('–¥–∞—Ç–∞_–æ—Å–Ω–æ–≤–∞–Ω–∏—è'):
                result += f"üìÖ {about['–¥–∞—Ç–∞_–æ—Å–Ω–æ–≤–∞–Ω–∏—è']}\n"
            if about.get('–º–∏—Å—Å–∏—è'):
                result += f"üéØ {about['–º–∏—Å—Å–∏—è'][:100]}...\n"
            if about.get('—Ü–µ–ª–µ–≤–∞—è_–∞—É–¥–∏—Ç–æ—Ä–∏—è'):
                result += f"üë• {about['—Ü–µ–ª–µ–≤–∞—è_–∞—É–¥–∏—Ç–æ—Ä–∏—è']}"
            return result
        
        return "üè´ –ù–î–¢ ‚Äî —Ü–µ–Ω—Ç—Ä —Ä–∞–∑–≤–∏—Ç–∏—è –æ–¥–∞—Ä—ë–Ω–Ω—ã—Ö –¥–µ—Ç–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ STEM"
    
    async def get_context_async(self, query: str) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è"""
        return self.get_context(query)
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        cache_stats = self.cache.get_stats()
        cache_hit_rate = (self.stats['cache_hits'] / max(1, self.stats['total_queries'])) * 100
        
        return {
            **self.stats,
            "cache_hit_rate": f"{cache_hit_rate:.1f}%",
            "avg_tokens_per_query": self.stats['tokens_saved'] / max(1, self.stats['total_queries']),
            **cache_stats
        }

class RAGModes:
    """–†–µ–∂–∏–º—ã —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤"""
    ULTRA_ECONOMY = 300    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è
    ECONOMY = 500         # –≠–∫–æ–Ω–æ–º–Ω—ã–π —Ä–µ–∂–∏–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    BALANCED = 800        # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
    DETAILED = 1200       # –ü–æ–¥—Ä–æ–±–Ω—ã–π

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
_rag_instances = {}

def get_optimized_rag(mode: int = RAGModes.ECONOMY) -> TokenOptimizedRAG:
    """–ü–æ–ª—É—á–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é RAG —Å–∏—Å—Ç–µ–º—É"""
    if mode not in _rag_instances:
        _rag_instances[mode] = TokenOptimizedRAG(max_tokens=mode)
    return _rag_instances[mode]

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
async def get_optimized_context_async(query: str, mode: int = RAGModes.ECONOMY) -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    rag = get_optimized_rag(mode)
    return await rag.get_context_async(query)

def get_optimized_context(query: str, mode: int = RAGModes.ECONOMY) -> str:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    rag = get_optimized_rag(mode)
    return rag.get_context(query)

# –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API
async def get_context_for_query_async(query: str) -> str:
    """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API"""
    return await get_optimized_context_async(query, RAGModes.ECONOMY)

def get_context_for_query(query: str) -> str:
    """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API"""
    return get_optimized_context(query, RAGModes.ECONOMY) 