import aiohttp
import logging
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Optional, List
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class DocumentsParser:
    """–ü–∞—Ä—Å–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è"""
    
    def __init__(self):
        self.url = "https://ndtp.by/for_incoming_students/"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        self.cache_file = "documents_cache.json"
        self.last_update_file = "last_documents_update.txt"
        self.base_url = "https://ndtp.by"
        
    async def fetch_page(self) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
        try:
            logger.info(f"üåê –ó–∞–ø—Ä–æ—Å –∫ {self.url}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.url, headers=self.headers, timeout=30) as response:
                    if response.status == 200:
                        content = await response.text()
                        logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        return content
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: HTTP {response.status}")
                        return None
                        
        except asyncio.TimeoutError:
            logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        
        return None
    
    def parse_documents_section(self, html_content: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç —Ä–∞–∑–¥–µ–ª —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ "–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã" (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—è–º–±–¥–∞ –∫–∞–∫ –≤ –≥–∞–π–¥–µ)
            documents_header = soup.find(
                lambda tag: tag.name in {"h1", "h2", "h3", "h4"} and
                           "–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã" in tag.get_text(strip=True).lower()
            )
            
            if not documents_header:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ '–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'")
                return {}
            
            logger.info("üìã –ù–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª '–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'")
            
            # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            documents_info = {
                "title": documents_header.get_text(strip=True),
                "items": [],
                "raw_html": str(documents_header),
                "last_updated": datetime.now().isoformat()
            }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º find_all_next() –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤—Å–µ—Ö —Å–ª–µ–¥—É—é—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            for element in documents_header.find_all_next():
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø—Ä–∏ –≤—Å—Ç—Ä–µ—á–µ –Ω–æ–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
                if element.name in {"h1", "h2", "h3", "h4"} and element is not documents_header:
                    logger.info(f"üìç –î–æ—Å—Ç–∏–≥–Ω—É—Ç —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {element.get_text(strip=True)[:50]}")
                    break
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
                if element.name == "ul":
                    logger.info(f"üìã –ù–∞–π–¥–µ–Ω —Å–ø–∏—Å–æ–∫ —Å {len(element.find_all('li', recursive=False))} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏")
                    for li in element.find_all("li", recursive=False):
                        item_info = self._parse_document_item(li)
                        if item_info:
                            documents_info["items"].append(item_info)
                            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {item_info['text'][:50]}...")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏
                elif element.name == "p":
                    paragraph_text = element.get_text(strip=True)
                    if paragraph_text and len(paragraph_text) > 10:
                        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–∞—Ä–∫–µ—Ä–æ–≤
                        lines = [
                            line.strip(" *‚Ä¢‚Äì\u2022").strip()
                            for line in element.get_text("\n").splitlines()
                            if line.strip(" *‚Ä¢‚Äì\u2022").strip()
                        ]
                        
                        if lines:
                            # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é
                            for line in lines:
                                if len(line) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                                    links = self._extract_links_from_element(element)
                                    item_info = {
                                        "type": "description",
                                        "text": line,
                                        "links": links,
                                        "document_type": "other"
                                    }
                                    documents_info["items"].append(item_info)
                                    logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ: {line[:50]}...")
                        else:
                            # –û–±—ã—á–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –±–µ–∑ –º–∞—Ä–∫–µ—Ä–æ–≤
                            links = self._extract_links_from_element(element)
                            item_info = {
                                "type": "description", 
                                "text": paragraph_text,
                                "links": links,
                                "document_type": "other"
                            }
                            documents_info["items"].append(item_info)
                            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–≥—Ä–∞—Ñ: {paragraph_text[:50]}...")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                documents_info["raw_html"] += str(element)
            
            logger.info(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents_info['items'])}")
            return documents_info
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return {}
    
    def _parse_document_item(self, li_element) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–±–µ–ª –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –¥–ª—è –ª—É—á—à–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            text = li_element.get_text(" ", strip=True)
            if not text or len(text) < 5:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
            links = self._extract_links_from_element(li_element)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_type = self._classify_document_type(text)
            
            logger.debug(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —ç–ª–µ–º–µ–Ω—Ç–∞: {text[:50]}... (—Å—Å—ã–ª–æ–∫: {len(links)})")
            
            return {
                "type": "document",
                "text": text,
                "links": links,
                "document_type": doc_type
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return None
    
    def _extract_links_from_element(self, element) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        links = []
        
        try:
            for link in element.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if href and text:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
                    if href.startswith('/'):
                        full_url = self.base_url + href
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        full_url = self.base_url + '/' + href
                    
                    links.append({
                        "text": text,
                        "url": full_url,
                        "original_href": href
                    })
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫: {e}")
        
        return links
    
    def _classify_document_type(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Ç–µ–∫—Å—Ç—É"""
        text_lower = text.lower()
        
        if '–∑–∞—è–≤–ª–µ–Ω–∏–µ' in text_lower:
            return 'application'
        elif '—Å–æ–≥–ª–∞—Å–∏–µ' in text_lower:
            return 'consent'
        elif '–ø–ª–∞–Ω' in text_lower and '—É—á–µ–±–Ω—ã–π' in text_lower:
            return 'study_plan'
        elif '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ' in text_lower and '—Ä–æ–∂–¥–µ–Ω–∏' in text_lower:
            return 'birth_certificate'
        elif '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è' in text_lower and '—Å–ø—Ä–∞–≤–∫–∞' in text_lower:
            return 'medical_certificate'
        elif '—Å–ø—Ä–∞–≤–∫–∞' in text_lower and '–±–∞—Å—Å–µ–π–Ω' in text_lower:
            return 'pool_certificate'
        elif '—Å–ø—Ä–∞–≤–∫–∞' in text_lower and '–∏–Ω—Ñ–µ–∫—Ü' in text_lower:
            return 'infection_certificate'
        else:
            return 'other'
    
    def save_documents_cache(self, documents_data: Dict) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤ –∫–µ—à"""
        try:
            cache_data = {
                "last_updated": datetime.now().isoformat(),
                "source_url": self.url,
                "documents": documents_data
            }
            
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            with open(self.last_update_file, 'w', encoding='utf-8') as f:
                f.write(datetime.now().isoformat())
            
            logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.cache_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö: {e}")
            return False
    
    def load_documents_cache(self) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏–∑ –∫–µ—à–∞"""
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.info("üìÑ –§–∞–π–ª –∫–µ—à–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return None
    
    def get_last_update_time(self) -> Optional[datetime]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
        try:
            with open(self.last_update_file, 'r', encoding='utf-8') as f:
                time_str = f.read().strip()
                return datetime.fromisoformat(time_str)
        except (FileNotFoundError, ValueError):
            return None
    
    def should_update(self, hours_threshold: int = 24) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∂–¥—ã–µ 24 —á–∞—Å–∞)"""
        last_update = self.get_last_update_time()
        if not last_update:
            return True
        
        time_diff = datetime.now() - last_update
        return time_diff > timedelta(hours=hours_threshold)
    
    async def update_documents(self, force: bool = False) -> bool:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
        try:
            if not force and not self.should_update():
                logger.info("‚è∞ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
                return True
            
            logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            html_content = await self.fetch_page()
            if not html_content:
                return False
            
            # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            documents_data = self.parse_documents_section(html_content)
            if not documents_data:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
                return False
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            if self.save_documents_cache(documents_data):
                logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {len(documents_data.get('items', []))} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                return True
            else:
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö: {e}")
            return False
    
    def get_documents_context(self, query: str = "") -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –¥–ª—è –ò–ò"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–≤—è–∑–∞–Ω –ª–∏ –∑–∞–ø—Ä–æ—Å —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            document_keywords = [
                '–¥–æ–∫—É–º–µ–Ω—Ç', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Å–ø—Ä–∞–≤–∫', '–∑–∞—è–≤–ª–µ–Ω', '—Å–æ–≥–ª–∞—Å–∏', '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ',
                '–º–µ–¥–∏—Ü–∏–Ω—Å–∫', '—Ä–æ–∂–¥–µ–Ω–∏', '–±–∞—Å—Å–µ–π–Ω', '–∏–Ω—Ñ–µ–∫—Ü', '–ø–ª–∞–Ω', '—É—á–µ–±–Ω—ã–π',
                '–ø—Ä–∏ –∑–∞–µ–∑–¥–µ', '–ø–æ—Å—Ç—É–ø–ª–µ–Ω', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü', '—á—Ç–æ –Ω—É–∂–Ω–æ', '—á—Ç–æ –≤–∑—è—Ç—å',
                '–∫–∞–∫–∏–µ –Ω—É–∂–Ω—ã', '—Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤', '–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ'
            ]
            
            query_lower = query.lower()
            is_documents_related = any(keyword in query_lower for keyword in document_keywords)
            
            if not is_documents_related and query:
                return ""
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–µ—à–∞
            cache_data = self.load_documents_cache()
            if not cache_data:
                return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
            
            documents_info = cache_data.get("documents", {})
            if not documents_info:
                return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_parts = [
                "üìÑ –ù–ï–û–ë–•–û–î–ò–ú–´–ï –î–û–ö–£–ú–ï–ù–¢–´ –î–õ–Ø –ü–û–°–¢–£–ü–õ–ï–ù–ò–Ø –í –ù–ê–¶–ò–û–ù–ê–õ–¨–ù–´–ô –î–ï–¢–°–ö–ò–ô –¢–ï–•–ù–û–ü–ê–†–ö",
                f"–ò—Å—Ç–æ—á–Ω–∏–∫: {self.url}",
                f"–û–±–Ω–æ–≤–ª–µ–Ω–æ: {cache_data['last_updated'][:16]}",
                ""
            ]
            
            for item in documents_info.get("items", []):
                if item["type"] == "document":
                    context_parts.append(f"‚Ä¢ {item['text']}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
                    for link in item.get("links", []):
                        context_parts.append(f"  üìé {link['text']}: {link['url']}")
                    
                elif item["type"] == "description":
                    context_parts.append(f"\n{item['text']}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                    for link in item.get("links", []):
                        context_parts.append(f"üìé {link['text']}: {link['url']}")
                
                context_parts.append("")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
documents_parser = DocumentsParser()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –±–æ—Ç–æ–º
async def get_documents_context_async(query: str = "") -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
    try:
        # –ü–æ–ø—Ä–æ–±—É–µ–º –æ–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        await documents_parser.update_documents()
        return documents_parser.get_documents_context(query)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        return documents_parser.get_documents_context(query)  # –ü–æ–ø—Ä–æ–±—É–µ–º —Å –∫–µ—à–µ–º

def get_documents_context(query: str = "") -> str:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
    return documents_parser.get_documents_context(query)

async def force_update_documents() -> bool:
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
    return await documents_parser.update_documents(force=True)

async def documents_updater_loop(interval_hours: int = 24):
    """–§–æ–Ω–æ–≤—ã–π —Ü–∏–∫–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
    logger.info(f"üîÑ –ó–∞–ø—É—â–µ–Ω —Ü–∏–∫–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∫–∞–∂–¥—ã–µ {interval_hours} —á–∞—Å–æ–≤)")
    
    while True:
        try:
            await asyncio.sleep(interval_hours * 60 * 60)  # –ñ–¥–µ–º –∑–∞–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤
            
            if documents_parser.should_update():
                logger.info("‚è∞ –ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
                success = await documents_parser.update_documents()
                if success:
                    logger.info("‚úÖ –ü–ª–∞–Ω–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
                else:
                    logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–ª–∞–Ω–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            else:
                logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∞–∫—Ç—É–∞–ª—å–Ω—ã")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            await asyncio.sleep(60 * 60)  # –ñ–¥–µ–º —á–∞—Å –ø—Ä–∏ –æ—à–∏–±–∫–µ 