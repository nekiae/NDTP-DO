import asyncio
import io
import json
import logging
import os
import re
from datetime import datetime

import PyPDF2
import requests
from bs4 import BeautifulSoup

from src.core.config import config

try:
    from pdf2image import convert_from_bytes
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("‚ö†Ô∏è OCR –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (pdf2image, pytesseract)")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

"""
üìã –ü–ê–†–°–ï–† –°–ü–ò–°–ö–û–í –¢–ï–•–ù–û–ü–ê–†–ö–ê

üîç –õ–û–ì–ò–ö–ê –ü–û–ò–°–ö–ê (–æ–±–Ω–æ–≤–ª–µ–Ω–æ):
‚Ä¢ –ü—Ä–∏ –ø–æ–∏—Å–∫–µ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ - –∏—â–µ—Ç —ç—Ç–æ —Å–ª–æ–≤–æ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ (—Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏)
‚Ä¢ –ü—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤ - –∏—â–µ—Ç –∏—Ö –¢–û–õ–¨–ö–û –∫–∞–∫ —Ü–µ–ª—É—é —Ñ—Ä–∞–∑—É
‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è –¥–≤—É—Ö —Å–ª–æ–≤ (–ò–º—è –§–∞–º–∏–ª–∏—è ‚Üî –§–∞–º–∏–ª–∏—è –ò–º—è)
‚Ä¢ –ù–ï –Ω–∞—Ö–æ–¥–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ —Å–ª–æ–≤–∞ –µ—Å—Ç—å –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö

‚úÖ –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:
‚Ä¢ "–ò–≤–∞–Ω–æ–≤ –ü–µ—Ç—Ä" ‚Üí –Ω–∞–π–¥–µ—Ç "–ò–≤–∞–Ω–æ–≤ –ü–µ—Ç—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á"
‚Ä¢ "–ü–µ—Ç—Ä –ò–≤–∞–Ω–æ–≤" ‚Üí –Ω–∞–π–¥–µ—Ç "–ò–≤–∞–Ω–æ–≤ –ü–µ—Ç—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á" (–æ–±—Ä–∞—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫)
‚Ä¢ "–ò–≤–∞–Ω–æ–≤" ‚Üí –Ω–∞–π–¥–µ—Ç –ª—é–±—ã–µ –∑–∞–ø–∏—Å–∏ —Å "–ò–≤–∞–Ω–æ–≤"

‚ùå –ß—Ç–æ –ù–ï –Ω–∞–π–¥–µ—Ç:
‚Ä¢ "–ò–≤–∞–Ω–æ–≤ –ê–Ω–Ω–∞" –Ω–µ –Ω–∞–π–¥–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç, –≥–¥–µ –µ—Å—Ç—å "–ò–≤–∞–Ω–æ–≤" –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ –∏ "–ê–Ω–Ω–∞" –≤ –¥—Ä—É–≥–æ–º
"""

# –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞
BASE_URL = "https://ndtp.by"
SCHEDULE_URL = f"{BASE_URL}/schedule/"

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
PDF_CACHE_FILE = config.cache_dir / 'lists_cache.json'
CACHE_LIFETIME = 3600  # 1 —á–∞—Å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
PDF_CACHE_EXPIRY = 86400  # 24 —á–∞—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

class ListsParser:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –ø–æ–∏—Å–∫–∞ –≤ —Å–ø–∏—Å–∫–∞—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–∞"""
    
    def __init__(self):
        self.ensure_cache_dir()
        self.pdf_cache = self.load_pdf_cache()
        self.last_update = None
    
    @staticmethod
    def ensure_cache_dir():
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫—ç—à–∞ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
        try:
            cache_dir = config.cache_dir
            if not cache_dir.exists():
                cache_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞: {cache_dir}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫—ç—à–∞: {e}")
    
    def load_pdf_cache(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ PDF-—Ñ–∞–π–ª–æ–≤"""
        try:
            if os.path.exists(PDF_CACHE_FILE):
                with open(PDF_CACHE_FILE, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à PDF-—Ñ–∞–π–ª–æ–≤: {len(cache_data)} —Å–º–µ–Ω")
                    return cache_data
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—ç—à–∞ PDF: {e}")
        return {}
    
    def save_pdf_cache(self, cache):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ PDF-—Ñ–∞–π–ª–æ–≤"""
        try:
            with open(PDF_CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ –ö—ç—à PDF-—Ñ–∞–π–ª–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(cache)} —Å–º–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞ PDF: {e}")
    
    async def get_shifts_info(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—Å–µ—Ö —Å–º–µ–Ω–∞—Ö —Å —Å–∞–π—Ç–∞"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            }
            
            logger.info(f"üåê –ó–∞–ø—Ä–æ—Å –∫ {SCHEDULE_URL}")
            response = requests.get(SCHEDULE_URL, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            shifts = []
            panels = soup.find_all('div', class_='panel-default')
            
            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ –ø–∞–Ω–µ–ª–µ–π: {len(panels)}")
            
            for panel in panels:
                try:
                    title_elem = panel.find(['h1', 'h2', 'h3', 'h4', 'h5'], 
                        string=lambda x: x and ('—Å–º–µ–Ω–∞' in x.lower() or '—Å–º–µ–Ω—ã' in x.lower()))
                    
                    if not title_elem:
                        continue
                    
                    shift_name = title_elem.text.strip()
                    panel_body = panel.find('div', class_='panel-body')
                    
                    if not panel_body:
                        continue
                    
                    # –ü–æ–∏—Å–∫ –¥–∞—Ç
                    dates = self._extract_dates(panel_body)
                    if not dates:
                        continue
                    
                    # –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞—è–≤–∫–∞—Ö
                    application_period = self._extract_application_info(panel_body)
                    if not application_period:
                        continue
                    
                    # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    documents = self._extract_documents(panel_body)
                    if not documents:
                        continue
                    
                    shifts.append({
                        'name': shift_name,
                        'dates': dates,
                        'application_period': application_period,
                        'documents': documents
                    })
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞–Ω–µ–ª–∏: {e}")
                    continue
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–º–µ–Ω: {len(shifts)}")
            return shifts
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–º–µ–Ω–∞—Ö: {e}")
            return []
    
    def _extract_dates(self, panel_body):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç –∏–∑ –ø–∞–Ω–µ–ª–∏"""
        date_patterns = [
            r'\d{2}[\.,-]\d{2}[\.,-]202\d',
            r'\d{2}[\.,-]\d{2}',
            r'\d+\s*[‚Äì-]\s*\d+',
        ]
        
        for pattern in date_patterns:
            for elem in panel_body.find_all(['h5', 'p', 'div']):
                if elem.text and re.search(pattern, elem.text):
                    return elem.text.strip()
        return None
    
    def _extract_application_info(self, panel_body):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–∏–µ–º–µ –∑–∞—è–≤–æ–∫"""
        for p in panel_body.find_all(['p', 'div']):
            text = p.text.lower()
            if any(word in text for word in ['–ø—Ä–∏–µ–º', '–∑–∞—è–≤–∫', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü', '–ø–æ–¥–∞—á']):
                return p.text.strip()
        return None
    
    def _extract_documents(self, panel_body):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–Ω–µ–ª–∏"""
        documents = []
        
        for link in panel_body.find_all('a', href=True):
            href = link['href']
            
            if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx']) or \
               any(keyword in href.lower() for keyword in ['download', 'media_dl', 'document']):
                doc_url = href if href.startswith('http') else f"{BASE_URL}{href}"
                
                doc_name = link.text.strip()
                if not doc_name and link.parent:
                    doc_name = link.parent.text.strip()
                if doc_name:
                    doc_name = ' '.join(doc_name.split())
                    if not any(d['url'] == doc_url for d in documents):
                        documents.append({
                            'name': doc_name,
                            'url': doc_url,
                            'type': self._detect_document_type(doc_name, doc_url)
                        })
        
        return documents
    
    def _detect_document_type(self, doc_name, doc_url):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        name_lower = doc_name.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        student_list_keywords = [
            '—Å–ø–∏—Å–æ–∫', '–∑–∞—á–∏—Å–ª–µ–Ω', '–≥—Ä—É–ø–ø', '—É—á–∞—Å—Ç–Ω–∏–∫', '–¥–æ–ø—É—â–µ–Ω', 
            '—Å–æ—Å—Ç–∞–≤', '—É—á–∞—â–∏', '–ø–æ—Å—Ç—É–ø–∏–≤—à', '–∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç',
            '–ø—Ä–∏–Ω—è—Ç', '–æ—Ç–æ–±—Ä–∞–Ω', '–∫–∞–Ω–¥–∏–¥–∞—Ç'
        ]
        
        if any(keyword in name_lower for keyword in student_list_keywords):
            return 'student_list'
        elif any(keyword in name_lower for keyword in ['–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–ø–ª–∞–Ω', '–æ–ø–∏—Å–∞–Ω–∏–µ']):
            return 'program'
        elif any(keyword in name_lower for keyword in ['–∑–∞—è–≤–ª–µ–Ω', '–∞–Ω–∫–µ—Ç', '—Ñ–æ—Ä–º']):
            return 'application'
        else:
            return 'other'
    
    async def extract_text_from_pdf(self, url, force_ocr=False):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF-—Ñ–∞–π–ª–∞"""
        try:
            logger.info(f"üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ PDF: {url}")
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            pdf_content = response.content
            text = ""
            
            # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
            try:
                with io.BytesIO(pdf_content) as file_stream:
                    reader = PyPDF2.PdfReader(file_stream)
                    for page in reader.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}")
            
            # –ï—Å–ª–∏ OCR –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –Ω—É–∂–µ–Ω
            if (force_ocr or len(text.strip()) < 100) and OCR_AVAILABLE:
                try:
                    logger.info("üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º OCR –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞")
                    images = convert_from_bytes(pdf_content)
                    ocr_text = ""
                    for i, image in enumerate(images):
                        logger.info(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {i+1}/{len(images)}")
                        page_text = pytesseract.image_to_string(image, lang='rus')
                        ocr_text += page_text + "\n"
                    
                    if len(ocr_text.strip()) > len(text.strip()):
                        text = ocr_text
                        logger.info(f"‚úÖ OCR –¥–∞–ª –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è OCR –Ω–µ —É–¥–∞–ª—Å—è: {e}")
            
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
            return text if text.strip() else None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF {url}: {e}")
            return None
    
    async def preload_pdf_files(self, force_reload=False):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ PDF-—Ñ–∞–π–ª–æ–≤"""
        try:
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É PDF-—Ñ–∞–π–ª–æ–≤...")
            
            current_time = datetime.now().timestamp()
            shifts = await self.get_shifts_info()
            
            if not shifts:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–º–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
                return self.pdf_cache
            
            total_docs = sum(len(shift['documents']) for shift in shifts if shift['documents'])
            processed_docs = 0
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(shifts)} —Å–º–µ–Ω, {total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            for shift in shifts:
                shift_name = shift['name']
                if shift_name not in self.pdf_cache:
                    self.pdf_cache[shift_name] = {}
                
                logger.info(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–º–µ–Ω—É: {shift_name}")
                
                for doc in shift['documents']:
                    processed_docs += 1
                    doc_url = doc['url']
                    doc_name = doc['name']
                    doc_type = doc['type']
                    
                    logger.info(f"[{processed_docs}/{total_docs}] {doc_name}")
                    
                    is_pdf = doc_url.lower().endswith('.pdf') or ('media_dl=' in doc_url.lower())
                    
                    if is_pdf:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
                        if not force_reload and doc_url in self.pdf_cache[shift_name]:
                            cached_doc = self.pdf_cache[shift_name][doc_url]
                            if current_time - cached_doc['timestamp'] < PDF_CACHE_EXPIRY:
                                logger.info("üìö –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é")
                                continue
                        
                        is_student_list = doc_type == 'student_list'
                        
                        try:
                            text = await self.extract_text_from_pdf(doc_url, force_ocr=is_student_list)
                            if text:
                                self.pdf_cache[shift_name][doc_url] = {
                                    'name': doc_name,
                                    'text': text,
                                    'timestamp': current_time,
                                    'is_student_list': is_student_list,
                                    'type': doc_type
                                }
                                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {doc_name}")
                            else:
                                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç: {doc_name}")
                        except Exception as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {doc_name}: {e}")
            
            self.save_pdf_cache(self.pdf_cache)
            self.last_update = datetime.now()
            
            total_cached = sum(len(docs) for docs in self.pdf_cache.values())
            logger.info(f"üéâ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ö—ç—à–∏—Ä–æ–≤–∞–Ω–æ {total_cached} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            return self.pdf_cache
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ PDF-—Ñ–∞–π–ª–æ–≤: {e}")
            return {}
    
    async def search_in_lists(self, query: str, search_type='all') -> list:
        """–ü–æ–∏—Å–∫ –≤ —Å–ø–∏—Å–∫–∞—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤"""
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}' (—Ç–∏–ø: {search_type})")
            
            if not query or not query.strip():
                return []
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
            query_lower = query.lower().strip()
            query_parts = query_lower.split()
            
            results = []
            total_docs = sum(len(docs) for docs in self.pdf_cache.values())
            processed = 0
            
            logger.info(f"üìä –ü–æ–∏—Å–∫ –≤ {total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
            
            for shift_name, docs in self.pdf_cache.items():
                for doc_url, doc_data in docs.items():
                    processed += 1
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø—É –ø–æ–∏—Å–∫–∞
                    if search_type == 'student_lists' and not doc_data.get('is_student_list', False):
                        continue
                    
                    doc_text = doc_data['text'].lower()
                    doc_name = doc_data['name']
                    
                    # –ü–æ–∏—Å–∫
                    found, match_info = self._search_in_text(query_lower, query_parts, doc_text)
                    
                    if found:
                        logger.info(f"‚úÖ –ù–ê–ô–î–ï–ù–û! {match_info} –≤ '{doc_name}' ({shift_name})")
                        
                        context = self._get_context(doc_text, query_lower, query_parts)
                        
                        result = {
                            'shift': shift_name,
                            'document': doc_name,
                            'url': doc_url,
                            'type': doc_data.get('type', 'other'),
                            'match_info': match_info,
                            'context': context,
                            'is_student_list': doc_data.get('is_student_list', False)
                        }
                        
                        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Å–ø–∏—Å–∫–æ–≤ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
                        if doc_data.get('is_student_list', False):
                            results.insert(0, result)
                        else:
                            results.append(result)
            
            logger.info(f"üéâ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _search_in_text(self, query, query_parts, text):
        """–ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
        # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ñ—Ä–∞–∑—ã
        if query in text:
            return True, "–ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"
        
        # 2. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ - –∏—â–µ–º —Ç–æ–ª—å–∫–æ —Ü–µ–ª—É—é —Ñ—Ä–∞–∑—É
        if len(query_parts) >= 2:
            # –£–±–∏—Ä–∞–µ–º –ª–æ–≥–∏–∫—É –ø–æ–∏—Å–∫–∞ –ø–æ —á–∞—Å—Ç—è–º - —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è (—Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏)
            import re
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–π –ø—Ä–æ–±–µ–ª–æ–≤
            pattern = r'\b' + r'\s+'.join(re.escape(part) for part in query_parts) + r'\b'
            
            if re.search(pattern, text, re.IGNORECASE):
                return True, f"–Ω–∞–π–¥–µ–Ω–∞ —Ñ—Ä–∞–∑–∞: {query}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ (–ò–º—è –§–∞–º–∏–ª–∏—è -> –§–∞–º–∏–ª–∏—è –ò–º—è)
            if len(query_parts) == 2:
                reversed_query = f"{query_parts[1]} {query_parts[0]}"
                if reversed_query in text:
                    return True, f"–Ω–∞–π–¥–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ (–æ–±—Ä–∞—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫): {reversed_query}"
                
                # –ò —Å regex –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
                reversed_pattern = r'\b' + re.escape(query_parts[1]) + r'\s+' + re.escape(query_parts[0]) + r'\b'
                if re.search(reversed_pattern, text, re.IGNORECASE):
                    return True, f"–Ω–∞–π–¥–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ (–æ–±—Ä–∞—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫): {reversed_query}"
        
        # 3. –û–¥–Ω–æ —Å–ª–æ–≤–æ (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω–æ–µ)
        elif len(query_parts) == 1 and len(query_parts[0]) >= 3:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            import re
            pattern = r'\b' + re.escape(query_parts[0]) + r'\b'
            if re.search(pattern, text, re.IGNORECASE):
                return True, f"–Ω–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤–æ: {query_parts[0]}"
        
        return False, ""
    
    def _get_context(self, text, query, query_parts, context_size=100):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        try:
            index = text.find(query)
            if index == -1 and query_parts:
                index = text.find(query_parts[0])
            
            if index >= 0:
                start = max(0, index - context_size)
                end = min(len(text), index + len(query) + context_size)
                context = text[start:end].strip()
                context = re.sub(r'\s+', ' ', context)
                return context[:200] + "..." if len(context) > 200 else context
            
            return ""
        except Exception:
            return ""
    
    def get_cache_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        try:
            total_shifts = len(self.pdf_cache)
            total_docs = sum(len(docs) for docs in self.pdf_cache.values())
            student_lists = 0
            
            for docs in self.pdf_cache.values():
                for doc_data in docs.values():
                    if doc_data.get('is_student_list', False):
                        student_lists += 1
            
            return {
                'total_shifts': total_shifts,
                'total_documents': total_docs,
                'student_lists': student_lists,
                'last_update': self.last_update.strftime('%Y-%m-%d %H:%M:%S') if self.last_update else '–ù–∏–∫–æ–≥–¥–∞',
                'cache_file_exists': os.path.exists(PDF_CACHE_FILE),
                'ocr_available': OCR_AVAILABLE
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}
    
    async def update_cache(self, force=False):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞"""
        try:
            logger.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞...")
            await self.preload_pdf_files(force_reload=force)
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è
lists_parser = ListsParser()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
async def search_name_in_lists(query: str, search_type='all'):
    """–ü–æ–∏—Å–∫ –∏–º–µ–Ω–∏ –≤ —Å–ø–∏—Å–∫–∞—Ö —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–∞"""
    return await lists_parser.search_in_lists(query, search_type)

async def update_lists_cache(force=False):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ —Å–ø–∏—Å–∫–æ–≤"""
    return await lists_parser.update_cache(force)

def get_lists_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ø–∏—Å–∫–æ–≤"""
    return lists_parser.get_cache_stats()

async def initialize_lists_parser():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å–ø–∏—Å–∫–æ–≤"""
    try:
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å–ø–∏—Å–∫–æ–≤...")
        stats = lists_parser.get_cache_stats()
        
        if stats.get('total_documents', 0) == 0:
            logger.info("üì• –ö—ç—à –ø—É—Å—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
            await lists_parser.preload_pdf_files()
        else:
            logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à: {stats['total_documents']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        return False

# –¢–µ—Å—Ç–æ–≤—ã–π –∫–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã –º–æ–¥—É–ª—è
if __name__ == "__main__":
    import asyncio
    
    async def test_module():
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ —Å–ø–∏—Å–∫–æ–≤...")
        
        # –¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        print("üìã –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
        success = await initialize_lists_parser()
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ' if success else '‚ùå'}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        stats = get_lists_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –ø–æ–∏—Å–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –∫—ç—à)
        if stats.get('total_documents', 0) > 0:
            print("\nüîç –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞...")
            test_queries = ["–ò–≤–∞–Ω–æ–≤", "–ü–µ—Ç—Ä–æ–≤–∞"]
            
            for query in test_queries:
                print(f"–ü–æ–∏—Å–∫: '{query}'")
                results = await search_name_in_lists(query)
                print(f"  –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç(–æ–≤)")
        
        print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç
    asyncio.run(test_module())